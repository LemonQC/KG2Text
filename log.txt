10/11/2021 19:52:33 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=200, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-06, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_9', no_lr_decay=False, num_beams=10, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_9/pos', predict_batch_size=16, predict_file='data/webnlg/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=32, train_file='data/webnlg/train', wait_step=30, warmup_proportion=0.01, warmup_steps=1000, weight_decay=0.0)
10/11/2021 19:52:33 - INFO - __main__ - out/jointgt_bart_webnlg_local
10/11/2021 19:52:33 - INFO - __main__ - Using 2 gpus
10/11/2021 19:52:33 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
10/11/2021 19:52:33 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
10/11/2021 19:52:33 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
10/11/2021 19:52:33 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
10/11/2021 19:52:33 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
10/11/2021 19:52:33 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
10/11/2021 19:52:33 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
10/11/2021 19:52:33 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 19:52:33 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 19:52:33 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 19:52:33 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 19:52:33 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_9/config.json
10/11/2021 19:52:33 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/11/2021 19:52:33 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_9/pytorch_model.bin
10/11/2021 19:52:39 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

10/11/2021 19:52:39 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
10/11/2021 19:52:39 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_9/pos/config.json
10/11/2021 19:52:39 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/11/2021 19:52:39 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_9/pos/pytorch_model.bin
10/11/2021 19:52:42 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BartForConditionalGeneration.

10/11/2021 19:52:42 - INFO - transformers.modeling_utils - All the weights of BartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_9/pos.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
10/11/2021 19:52:46 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=200, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-06, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_9', no_lr_decay=False, num_beams=10, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_9/pos', predict_batch_size=16, predict_file='data/webnlg/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=32, train_file='data/webnlg/train', wait_step=30, warmup_proportion=0.01, warmup_steps=1000, weight_decay=0.0)
10/11/2021 19:52:46 - INFO - __main__ - out/jointgt_bart_webnlg_local
10/11/2021 19:52:46 - INFO - __main__ - Using 2 gpus
10/11/2021 19:52:46 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
10/11/2021 19:52:46 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
10/11/2021 19:52:46 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
10/11/2021 19:52:46 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
10/11/2021 19:52:46 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
10/11/2021 19:52:46 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
10/11/2021 19:52:46 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
10/11/2021 19:52:46 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 19:52:46 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 19:52:46 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 19:52:46 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 19:52:46 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_9/config.json
10/11/2021 19:52:46 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/11/2021 19:52:46 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_9/pytorch_model.bin
10/11/2021 19:52:53 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

10/11/2021 19:52:53 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
10/11/2021 19:52:53 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_9/pos/config.json
10/11/2021 19:52:53 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/11/2021 19:52:53 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_9/pos/pytorch_model.bin
10/11/2021 19:52:55 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BartForConditionalGeneration.

10/11/2021 19:52:55 - INFO - transformers.modeling_utils - All the weights of BartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_9/pos.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
10/11/2021 19:52:57 - INFO - __main__ - Starting training!
10/11/2021 19:54:20 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=200, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-06, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_9', no_lr_decay=False, num_beams=10, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_9/pos', predict_batch_size=16, predict_file='data/webnlg/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=32, train_file='data/webnlg/train', wait_step=30, warmup_proportion=0.01, warmup_steps=1000, weight_decay=0.0)
10/11/2021 19:54:20 - INFO - __main__ - out/jointgt_bart_webnlg_local
10/11/2021 19:54:20 - INFO - __main__ - Using 2 gpus
10/11/2021 19:54:20 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
10/11/2021 19:54:20 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
10/11/2021 19:54:20 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
10/11/2021 19:54:20 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
10/11/2021 19:54:20 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
10/11/2021 19:54:20 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
10/11/2021 19:54:20 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
10/11/2021 19:54:20 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 19:54:20 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 19:54:20 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 19:54:20 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 19:54:20 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_9/config.json
10/11/2021 19:54:20 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/11/2021 19:54:20 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_9/pytorch_model.bin
10/11/2021 19:54:27 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

10/11/2021 19:54:27 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
10/11/2021 19:54:27 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_9/pos/config.json
10/11/2021 19:54:27 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/11/2021 19:54:27 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_9/pos/pytorch_model.bin
10/11/2021 19:54:30 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BartForConditionalGeneration.

10/11/2021 19:54:30 - INFO - transformers.modeling_utils - All the weights of BartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_9/pos.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
10/11/2021 19:54:33 - INFO - __main__ - Starting training!
10/11/2021 20:02:47 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_0_200.txt
10/11/2021 20:02:49 - INFO - __main__ - 当前的评估值: bleu-4:8.817759648266229, ROUGE_L:31.97016790986223, CIDEr:0.028117769756660103, Chrf:0.2475941043909518
10/11/2021 20:02:49 - INFO - __main__ - Step 200 Train loss 7.62 Learning rate 2.00e-06 BLEU 881.78% on epoch=0
10/11/2021 20:02:49 - INFO - __main__ - 当前最好的：-1.00
10/11/2021 20:02:49 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/config.json
10/11/2021 20:02:51 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pytorch_model.bin
10/11/2021 20:02:51 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/pos/config.json
10/11/2021 20:02:52 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pos/pytorch_model.bin
10/11/2021 20:02:52 - INFO - __main__ - Saving model with best BLEU: -100.00% -> 881.78% on epoch=0, global_step=200
10/11/2021 20:08:15 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_1_400.txt
10/11/2021 20:08:17 - INFO - __main__ - 当前的评估值: bleu-4:2.9235352556827654, ROUGE_L:31.948581557005294, CIDEr:0.007269921106825235, Chrf:0.2339334410959491
10/11/2021 20:08:17 - INFO - __main__ - Step 400 Train loss 4.62 Learning rate 4.00e-06 BLEU 292.35% on epoch=1
10/11/2021 20:08:17 - INFO - __main__ - 当前最好的：8.82
10/11/2021 20:13:51 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_2_600.txt
10/11/2021 20:13:52 - INFO - __main__ - 当前的评估值: bleu-4:3.7415378613673598, ROUGE_L:31.95397907513957, CIDEr:0.0063936244566629675, Chrf:0.23456997253570058
10/11/2021 20:13:52 - INFO - __main__ - Step 600 Train loss 3.94 Learning rate 6.00e-06 BLEU 374.15% on epoch=1
10/11/2021 20:13:52 - INFO - __main__ - 当前最好的：8.82
10/11/2021 20:24:16 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_3_800.txt
10/11/2021 20:24:19 - INFO - __main__ - 当前的评估值: bleu-4:10.429631604965714, ROUGE_L:30.721522346737945, CIDEr:0.017875324881684147, Chrf:0.2660374181944362
10/11/2021 20:24:19 - INFO - __main__ - Step 800 Train loss 3.60 Learning rate 8.00e-06 BLEU 1042.96% on epoch=2
10/11/2021 20:24:19 - INFO - __main__ - 当前最好的：8.82
10/11/2021 20:24:19 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_3/config.json
10/11/2021 20:24:20 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_3/pytorch_model.bin
10/11/2021 20:24:20 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_3/pos/config.json
10/11/2021 20:24:21 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_3/pos/pytorch_model.bin
10/11/2021 20:24:21 - INFO - __main__ - Saving model with best BLEU: 881.78% -> 1042.96% on epoch=2, global_step=800
10/11/2021 20:37:41 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_4_1000.txt
10/11/2021 20:37:46 - INFO - __main__ - 当前的评估值: bleu-4:4.968528428714735, ROUGE_L:24.82608154693004, CIDEr:0.01072990377119645, Chrf:0.28137514986733364
10/11/2021 20:37:46 - INFO - __main__ - Step 1000 Train loss 3.32 Learning rate 1.00e-05 BLEU 496.85% on epoch=2
10/11/2021 20:37:46 - INFO - __main__ - 当前最好的：10.43
10/11/2021 20:50:48 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_5_1200.txt
10/11/2021 20:50:54 - INFO - __main__ - 当前的评估值: bleu-4:5.213296382929742, ROUGE_L:24.71773525751841, CIDEr:0.01223839863126114, Chrf:0.2784353047992718
10/11/2021 20:50:54 - INFO - __main__ - Step 1200 Train loss 3.10 Learning rate 9.94e-06 BLEU 521.33% on epoch=3
10/11/2021 20:50:54 - INFO - __main__ - 当前最好的：10.43
10/11/2021 21:04:14 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_6_1400.txt
10/11/2021 21:04:19 - INFO - __main__ - 当前的评估值: bleu-4:5.056468209140468, ROUGE_L:24.28844051024223, CIDEr:0.010967298337375626, Chrf:0.2735400177593141
10/11/2021 21:04:19 - INFO - __main__ - Step 1400 Train loss 2.95 Learning rate 9.88e-06 BLEU 505.65% on epoch=3
10/11/2021 21:04:19 - INFO - __main__ - 当前最好的：10.43
10/11/2021 21:05:00 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=800, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-06, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_9', no_lr_decay=False, num_beams=10, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_9/pos', predict_batch_size=16, predict_file='data/webnlg/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=32, train_file='data/webnlg/train', wait_step=30, warmup_proportion=0.01, warmup_steps=2000, weight_decay=0.0)
10/11/2021 21:05:00 - INFO - __main__ - out/jointgt_bart_webnlg_local
10/11/2021 21:05:00 - INFO - __main__ - Using 2 gpus
10/11/2021 21:05:00 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
10/11/2021 21:05:00 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
10/11/2021 21:05:00 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
10/11/2021 21:05:00 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
10/11/2021 21:05:00 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
10/11/2021 21:05:00 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
10/11/2021 21:05:00 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
10/11/2021 21:05:00 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 21:05:00 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 21:05:00 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 21:05:00 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 21:05:00 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_9/config.json
10/11/2021 21:05:00 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/11/2021 21:05:00 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_9/pytorch_model.bin
10/11/2021 21:05:09 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

10/11/2021 21:05:09 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
10/11/2021 21:05:09 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_9/pos/config.json
10/11/2021 21:05:09 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/11/2021 21:05:09 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_9/pos/pytorch_model.bin
10/11/2021 21:05:13 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=800, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-06, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_9', no_lr_decay=False, num_beams=10, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_9/pos', predict_batch_size=16, predict_file='data/webnlg/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=32, train_file='data/webnlg/train', wait_step=30, warmup_proportion=0.01, warmup_steps=2000, weight_decay=0.0)
10/11/2021 21:05:13 - INFO - __main__ - out/jointgt_bart_webnlg_local
10/11/2021 21:05:13 - INFO - __main__ - Using 2 gpus
10/11/2021 21:05:13 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
10/11/2021 21:05:13 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
10/11/2021 21:05:13 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
10/11/2021 21:05:13 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
10/11/2021 21:05:13 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
10/11/2021 21:05:13 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
10/11/2021 21:05:13 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
10/11/2021 21:05:13 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 21:05:13 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 21:05:13 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 21:05:13 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 21:05:13 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_9/config.json
10/11/2021 21:05:13 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/11/2021 21:05:13 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_9/pytorch_model.bin
10/11/2021 21:05:21 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

10/11/2021 21:05:21 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
10/11/2021 21:05:22 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_9/pos/config.json
10/11/2021 21:05:22 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/11/2021 21:05:22 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_9/pos/pytorch_model.bin
10/11/2021 21:05:26 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BartForConditionalGeneration.

10/11/2021 21:05:26 - INFO - transformers.modeling_utils - All the weights of BartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_9/pos.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
10/11/2021 21:05:28 - INFO - __main__ - Starting training!
10/11/2021 21:23:10 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_0_800.txt
10/11/2021 21:23:13 - INFO - __main__ - 当前的评估值: bleu-4:60.72644692266208, ROUGE_L:78.97701018776158, CIDEr:5.209459622638824, Chrf:0.7903397635840651
10/11/2021 21:23:13 - INFO - __main__ - Step 800 Train loss 0.43 Learning rate 4.00e-06 BLEU 6072.64% on epoch=2
10/11/2021 21:23:13 - INFO - __main__ - 当前最好的：-1.00
10/11/2021 21:23:13 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/config.json
10/11/2021 21:23:14 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pytorch_model.bin
10/11/2021 21:23:14 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/pos/config.json
10/11/2021 21:23:14 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pos/pytorch_model.bin
10/11/2021 21:23:14 - INFO - __main__ - Saving model with best BLEU: -100.00% -> 6072.64% on epoch=2, global_step=800
10/11/2021 21:23:41 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=200, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-06, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_9', no_lr_decay=False, num_beams=10, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_9/pos', predict_batch_size=16, predict_file='data/webnlg/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=32, train_file='data/webnlg/train', wait_step=30, warmup_proportion=0.01, warmup_steps=2000, weight_decay=0.0)
10/11/2021 21:23:41 - INFO - __main__ - out/jointgt_bart_webnlg_local
10/11/2021 21:23:41 - INFO - __main__ - Using 2 gpus
10/11/2021 21:23:41 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
10/11/2021 21:23:41 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
10/11/2021 21:23:41 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
10/11/2021 21:23:41 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
10/11/2021 21:23:41 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
10/11/2021 21:23:41 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
10/11/2021 21:23:41 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
10/11/2021 21:23:41 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 21:23:41 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 21:23:41 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 21:23:41 - INFO - transformers.tokenization_utils_base - loading file None
10/11/2021 21:23:42 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_9/config.json
10/11/2021 21:23:42 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/11/2021 21:23:42 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_9/pytorch_model.bin
10/11/2021 21:23:50 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

10/11/2021 21:23:50 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
10/11/2021 21:23:50 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_9/pos/config.json
10/11/2021 21:23:50 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/11/2021 21:23:50 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_9/pos/pytorch_model.bin
10/11/2021 21:23:54 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BartForConditionalGeneration.

10/11/2021 21:23:54 - INFO - transformers.modeling_utils - All the weights of BartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_9/pos.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
10/11/2021 21:23:56 - INFO - __main__ - Starting training!
10/11/2021 21:30:22 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_0_200.txt
10/11/2021 21:30:25 - INFO - __main__ - 当前的评估值: bleu-4:61.14789451962099, ROUGE_L:79.0734699397697, CIDEr:5.261593850814021, Chrf:0.7925345071388166
10/11/2021 21:30:25 - INFO - __main__ - Step 200 Train loss 0.43 Learning rate 1.00e-06 BLEU 6114.79% on epoch=0
10/11/2021 21:30:25 - INFO - __main__ - 当前最好的：-1.00
10/11/2021 21:30:25 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/config.json
10/11/2021 21:30:29 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pytorch_model.bin
10/11/2021 21:30:29 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/pos/config.json
10/11/2021 21:30:33 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pos/pytorch_model.bin
10/11/2021 21:30:33 - INFO - __main__ - Saving model with best BLEU: -100.00% -> 6114.79% on epoch=0, global_step=200
10/11/2021 21:36:43 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_1_400.txt
10/11/2021 21:36:46 - INFO - __main__ - 当前的评估值: bleu-4:60.99616207601821, ROUGE_L:79.02480423009993, CIDEr:5.233389771267193, Chrf:0.7907617638481246
10/11/2021 21:36:46 - INFO - __main__ - Step 400 Train loss 0.43 Learning rate 2.00e-06 BLEU 6099.62% on epoch=1
10/11/2021 21:36:46 - INFO - __main__ - 当前最好的：61.15
10/11/2021 21:43:11 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_2_600.txt
10/11/2021 21:43:14 - INFO - __main__ - 当前的评估值: bleu-4:61.20143877393964, ROUGE_L:79.1399154392234, CIDEr:5.251644190321233, Chrf:0.7920338329006205
10/11/2021 21:43:14 - INFO - __main__ - Step 600 Train loss 0.42 Learning rate 3.00e-06 BLEU 6120.14% on epoch=1
10/11/2021 21:43:14 - INFO - __main__ - 当前最好的：61.15
10/11/2021 21:43:14 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_2/config.json
10/11/2021 21:43:14 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_2/pytorch_model.bin
10/11/2021 21:43:14 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_2/pos/config.json
10/11/2021 21:43:15 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_2/pos/pytorch_model.bin
10/11/2021 21:43:15 - INFO - __main__ - Saving model with best BLEU: 6114.79% -> 6120.14% on epoch=1, global_step=600
10/11/2021 21:49:44 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_3_800.txt
10/11/2021 21:49:47 - INFO - __main__ - 当前的评估值: bleu-4:60.86527556093083, ROUGE_L:78.97327887698748, CIDEr:5.223615629545604, Chrf:0.7900889583490887
10/11/2021 21:49:47 - INFO - __main__ - Step 800 Train loss 0.42 Learning rate 4.00e-06 BLEU 6086.53% on epoch=2
10/11/2021 21:49:47 - INFO - __main__ - 当前最好的：61.20
10/11/2021 21:56:16 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_4_1000.txt
10/11/2021 21:56:19 - INFO - __main__ - 当前的评估值: bleu-4:60.89417550596823, ROUGE_L:79.0612997726012, CIDEr:5.226955762433052, Chrf:0.7910613140335995
10/11/2021 21:56:19 - INFO - __main__ - Step 1000 Train loss 0.41 Learning rate 5.00e-06 BLEU 6089.42% on epoch=2
10/11/2021 21:56:19 - INFO - __main__ - 当前最好的：61.20
10/11/2021 22:02:49 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_5_1200.txt
10/11/2021 22:02:52 - INFO - __main__ - 当前的评估值: bleu-4:60.565962717131406, ROUGE_L:78.73808937897029, CIDEr:5.203503373264848, Chrf:0.7881906778797112
10/11/2021 22:02:52 - INFO - __main__ - Step 1200 Train loss 0.39 Learning rate 6.00e-06 BLEU 6056.60% on epoch=3
10/11/2021 22:02:52 - INFO - __main__ - 当前最好的：61.20
10/11/2021 22:09:22 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_6_1400.txt
10/11/2021 22:09:25 - INFO - __main__ - 当前的评估值: bleu-4:60.68709040016219, ROUGE_L:78.80017653761033, CIDEr:5.1841397894325345, Chrf:0.7896407450790871
10/11/2021 22:09:25 - INFO - __main__ - Step 1400 Train loss 0.39 Learning rate 7.00e-06 BLEU 6068.71% on epoch=3
10/11/2021 22:09:25 - INFO - __main__ - 当前最好的：61.20
10/11/2021 22:15:55 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_7_1600.txt
10/11/2021 22:15:59 - INFO - __main__ - 当前的评估值: bleu-4:60.61242252306411, ROUGE_L:78.8614814578433, CIDEr:5.211790769649602, Chrf:0.7893292881337006
10/11/2021 22:15:59 - INFO - __main__ - Step 1600 Train loss 0.37 Learning rate 8.00e-06 BLEU 6061.24% on epoch=4
10/11/2021 22:15:59 - INFO - __main__ - 当前最好的：61.20
10/11/2021 22:22:17 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_8_1800.txt
10/11/2021 22:22:20 - INFO - __main__ - 当前的评估值: bleu-4:60.27603817700704, ROUGE_L:78.70370598518801, CIDEr:5.151804383078989, Chrf:0.7872463284639445
10/11/2021 22:22:20 - INFO - __main__ - Step 1800 Train loss 0.36 Learning rate 9.00e-06 BLEU 6027.60% on epoch=5
10/11/2021 22:22:20 - INFO - __main__ - 当前最好的：61.20
10/11/2021 22:28:28 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_9_2000.txt
10/11/2021 22:28:31 - INFO - __main__ - 当前的评估值: bleu-4:60.194280849855886, ROUGE_L:78.38845484446716, CIDEr:5.146993417429114, Chrf:0.7860688385630474
10/11/2021 22:28:31 - INFO - __main__ - Step 2000 Train loss 0.36 Learning rate 1.00e-05 BLEU 6019.43% on epoch=5
10/11/2021 22:28:31 - INFO - __main__ - 当前最好的：61.20
10/11/2021 22:34:40 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_10_2200.txt
10/11/2021 22:34:43 - INFO - __main__ - 当前的评估值: bleu-4:59.75285580246851, ROUGE_L:78.19958334634383, CIDEr:5.100986945638683, Chrf:0.7840411737336019
10/11/2021 22:34:43 - INFO - __main__ - Step 2200 Train loss 0.38 Learning rate 9.94e-06 BLEU 5975.29% on epoch=6
10/11/2021 22:34:43 - INFO - __main__ - 当前最好的：61.20
10/11/2021 22:40:52 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_11_2400.txt
10/11/2021 22:40:55 - INFO - __main__ - 当前的评估值: bleu-4:60.33363061605696, ROUGE_L:78.49998593337035, CIDEr:5.1436227286176806, Chrf:0.7883908409636777
10/11/2021 22:40:55 - INFO - __main__ - Step 2400 Train loss 0.38 Learning rate 9.88e-06 BLEU 6033.36% on epoch=6
10/11/2021 22:40:55 - INFO - __main__ - 当前最好的：61.20
10/11/2021 22:47:07 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_12_2600.txt
10/11/2021 22:47:10 - INFO - __main__ - 当前的评估值: bleu-4:59.74094608072832, ROUGE_L:78.21369871236053, CIDEr:5.068168315677057, Chrf:0.7847818921370304
10/11/2021 22:47:10 - INFO - __main__ - Step 2600 Train loss 0.37 Learning rate 9.82e-06 BLEU 5974.09% on epoch=7
10/11/2021 22:47:10 - INFO - __main__ - 当前最好的：61.20
10/11/2021 22:53:17 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_13_2800.txt
10/11/2021 22:53:20 - INFO - __main__ - 当前的评估值: bleu-4:59.83995580252363, ROUGE_L:78.31012999665495, CIDEr:5.098012155776987, Chrf:0.784703288482157
10/11/2021 22:53:20 - INFO - __main__ - Step 2800 Train loss 0.36 Learning rate 9.76e-06 BLEU 5984.00% on epoch=7
10/11/2021 22:53:20 - INFO - __main__ - 当前最好的：61.20
10/11/2021 22:59:31 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_14_3000.txt
10/11/2021 22:59:33 - INFO - __main__ - 当前的评估值: bleu-4:59.232195229912364, ROUGE_L:77.99167309957664, CIDEr:5.002304270585535, Chrf:0.7823995627881523
10/11/2021 22:59:33 - INFO - __main__ - Step 3000 Train loss 0.35 Learning rate 9.70e-06 BLEU 5923.22% on epoch=8
10/11/2021 22:59:33 - INFO - __main__ - 当前最好的：61.20
10/11/2021 23:05:59 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_15_3200.txt
10/11/2021 23:06:03 - INFO - __main__ - 当前的评估值: bleu-4:59.63633569697604, ROUGE_L:78.07041279531691, CIDEr:5.060712890246852, Chrf:0.783274114928391
10/11/2021 23:06:03 - INFO - __main__ - Step 3200 Train loss 0.35 Learning rate 9.64e-06 BLEU 5963.63% on epoch=9
10/11/2021 23:06:03 - INFO - __main__ - 当前最好的：61.20
10/11/2021 23:12:19 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_16_3400.txt
10/11/2021 23:12:23 - INFO - __main__ - 当前的评估值: bleu-4:59.50998221058684, ROUGE_L:78.14967488880258, CIDEr:5.062202142615383, Chrf:0.783346375420562
10/11/2021 23:12:23 - INFO - __main__ - Step 3400 Train loss 0.34 Learning rate 9.58e-06 BLEU 5951.00% on epoch=9
10/11/2021 23:12:23 - INFO - __main__ - 当前最好的：61.20
10/11/2021 23:18:42 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_17_3600.txt
10/11/2021 23:18:44 - INFO - __main__ - 当前的评估值: bleu-4:59.74421092515201, ROUGE_L:78.19870240624128, CIDEr:5.084023617693495, Chrf:0.784838190935469
10/11/2021 23:18:44 - INFO - __main__ - Step 3600 Train loss 0.34 Learning rate 9.52e-06 BLEU 5974.42% on epoch=10
10/11/2021 23:18:44 - INFO - __main__ - 当前最好的：61.20
10/11/2021 23:25:08 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_18_3800.txt
10/11/2021 23:25:10 - INFO - __main__ - 当前的评估值: bleu-4:59.48235219425594, ROUGE_L:78.13814399674911, CIDEr:5.049457123056794, Chrf:0.7829704591405648
10/11/2021 23:25:10 - INFO - __main__ - Step 3800 Train loss 0.33 Learning rate 9.46e-06 BLEU 5948.24% on epoch=10
10/11/2021 23:25:10 - INFO - __main__ - 当前最好的：61.20
10/11/2021 23:31:33 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_19_4000.txt
10/11/2021 23:31:37 - INFO - __main__ - 当前的评估值: bleu-4:59.837083644258016, ROUGE_L:78.22574469600126, CIDEr:5.05533231801684, Chrf:0.7854324698182126
10/11/2021 23:31:37 - INFO - __main__ - Step 4000 Train loss 0.33 Learning rate 9.40e-06 BLEU 5983.71% on epoch=11
10/11/2021 23:31:37 - INFO - __main__ - 当前最好的：61.20
10/11/2021 23:37:53 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_20_4200.txt
10/11/2021 23:37:57 - INFO - __main__ - 当前的评估值: bleu-4:59.748840782087086, ROUGE_L:78.04956375309231, CIDEr:5.04826657562228, Chrf:0.7847336039602092
10/11/2021 23:37:57 - INFO - __main__ - Step 4200 Train loss 0.33 Learning rate 9.34e-06 BLEU 5974.88% on epoch=11
10/11/2021 23:37:57 - INFO - __main__ - 当前最好的：61.20
10/11/2021 23:44:12 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_21_4400.txt
10/11/2021 23:44:15 - INFO - __main__ - 当前的评估值: bleu-4:59.26188757711161, ROUGE_L:77.6675080355039, CIDEr:4.974680508821363, Chrf:0.7815977297829635
10/11/2021 23:44:15 - INFO - __main__ - Step 4400 Train loss 0.32 Learning rate 9.28e-06 BLEU 5926.19% on epoch=12
10/11/2021 23:44:15 - INFO - __main__ - 当前最好的：61.20
10/11/2021 23:50:27 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_22_4600.txt
10/11/2021 23:50:29 - INFO - __main__ - 当前的评估值: bleu-4:59.120855256885775, ROUGE_L:77.8738337915857, CIDEr:5.004400915860762, Chrf:0.779850314397394
10/11/2021 23:50:29 - INFO - __main__ - Step 4600 Train loss 0.32 Learning rate 9.22e-06 BLEU 5912.09% on epoch=13
10/11/2021 23:50:29 - INFO - __main__ - 当前最好的：61.20
10/11/2021 23:56:40 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_23_4800.txt
10/11/2021 23:56:42 - INFO - __main__ - 当前的评估值: bleu-4:59.15818351604572, ROUGE_L:77.83700053892692, CIDEr:4.979521230505453, Chrf:0.7804250651634137
10/11/2021 23:56:42 - INFO - __main__ - Step 4800 Train loss 0.31 Learning rate 9.16e-06 BLEU 5915.82% on epoch=13
10/11/2021 23:56:42 - INFO - __main__ - 当前最好的：61.20
10/12/2021 00:02:52 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_24_5000.txt
10/12/2021 00:02:55 - INFO - __main__ - 当前的评估值: bleu-4:59.44661278175537, ROUGE_L:78.16391749757815, CIDEr:5.033753736244814, Chrf:0.7833124080379896
10/12/2021 00:02:55 - INFO - __main__ - Step 5000 Train loss 0.31 Learning rate 9.10e-06 BLEU 5944.66% on epoch=14
10/12/2021 00:02:55 - INFO - __main__ - 当前最好的：61.20
10/12/2021 00:09:05 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_25_5200.txt
10/12/2021 00:09:08 - INFO - __main__ - 当前的评估值: bleu-4:59.72819498279672, ROUGE_L:78.03534494767003, CIDEr:5.054573747525143, Chrf:0.7839949325995206
10/12/2021 00:09:08 - INFO - __main__ - Step 5200 Train loss 0.30 Learning rate 9.04e-06 BLEU 5972.82% on epoch=14
10/12/2021 00:09:08 - INFO - __main__ - 当前最好的：61.20
10/12/2021 00:15:20 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_26_5400.txt
10/12/2021 00:15:22 - INFO - __main__ - 当前的评估值: bleu-4:59.24385139876169, ROUGE_L:77.64387289903073, CIDEr:4.974454728228897, Chrf:0.7823032987271873
10/12/2021 00:15:22 - INFO - __main__ - Step 5400 Train loss 0.30 Learning rate 8.98e-06 BLEU 5924.39% on epoch=15
10/12/2021 00:15:22 - INFO - __main__ - 当前最好的：61.20
10/12/2021 00:21:30 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_27_5600.txt
10/12/2021 00:21:32 - INFO - __main__ - 当前的评估值: bleu-4:58.97278651946567, ROUGE_L:77.68640706953849, CIDEr:4.960719074650024, Chrf:0.7791156494242051
10/12/2021 00:21:32 - INFO - __main__ - Step 5600 Train loss 0.30 Learning rate 8.92e-06 BLEU 5897.28% on epoch=15
10/12/2021 00:21:32 - INFO - __main__ - 当前最好的：61.20
10/12/2021 00:27:40 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_28_5800.txt
10/12/2021 00:27:42 - INFO - __main__ - 当前的评估值: bleu-4:59.02610955596755, ROUGE_L:77.55989836621315, CIDEr:4.9768337200974315, Chrf:0.7800219890472008
10/12/2021 00:27:42 - INFO - __main__ - Step 5800 Train loss 0.29 Learning rate 8.86e-06 BLEU 5902.61% on epoch=16
10/12/2021 00:27:42 - INFO - __main__ - 当前最好的：61.20
10/12/2021 00:33:50 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_29_6000.txt
10/12/2021 00:33:53 - INFO - __main__ - 当前的评估值: bleu-4:58.574778814644134, ROUGE_L:77.32821941269899, CIDEr:4.9330018284275665, Chrf:0.7765136990157471
10/12/2021 00:33:53 - INFO - __main__ - Step 6000 Train loss 0.29 Learning rate 8.80e-06 BLEU 5857.48% on epoch=17
10/12/2021 00:33:53 - INFO - __main__ - 当前最好的：61.20
10/12/2021 00:40:02 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_30_6200.txt
10/12/2021 00:40:05 - INFO - __main__ - 当前的评估值: bleu-4:59.05752846089815, ROUGE_L:77.6678356103371, CIDEr:4.965133960528337, Chrf:0.7812747982420515
10/12/2021 00:40:05 - INFO - __main__ - Step 6200 Train loss 0.29 Learning rate 8.73e-06 BLEU 5905.75% on epoch=17
10/12/2021 00:40:05 - INFO - __main__ - 当前最好的：61.20
10/12/2021 00:46:15 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_31_6400.txt
10/12/2021 00:46:17 - INFO - __main__ - 当前的评估值: bleu-4:58.90520967976669, ROUGE_L:77.52318741786091, CIDEr:4.946670482326278, Chrf:0.7798059267189161
10/12/2021 00:46:17 - INFO - __main__ - Step 6400 Train loss 0.29 Learning rate 8.67e-06 BLEU 5890.52% on epoch=18
10/12/2021 00:46:17 - INFO - __main__ - 当前最好的：61.20
10/12/2021 00:52:26 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_32_6600.txt
10/12/2021 00:52:28 - INFO - __main__ - 当前的评估值: bleu-4:59.069644668815634, ROUGE_L:77.50980615257285, CIDEr:4.942194058351927, Chrf:0.780627256250022
10/12/2021 00:52:28 - INFO - __main__ - Step 6600 Train loss 0.28 Learning rate 8.61e-06 BLEU 5906.96% on epoch=18
10/12/2021 00:52:28 - INFO - __main__ - 当前最好的：61.20
10/12/2021 08:39:26 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=200, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-06, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_2', no_lr_decay=False, num_beams=10, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_2/pos', predict_batch_size=16, predict_file='data/webnlg/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=34, train_file='data/webnlg/train', wait_step=30, warmup_proportion=0.01, warmup_steps=2000, weight_decay=0.0)
10/12/2021 08:39:26 - INFO - __main__ - out/jointgt_bart_webnlg_local
10/12/2021 08:39:26 - INFO - __main__ - Using 2 gpus
10/12/2021 08:39:26 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
10/12/2021 08:39:26 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
10/12/2021 08:39:26 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
10/12/2021 08:39:26 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
10/12/2021 08:39:26 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
10/12/2021 08:39:26 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
10/12/2021 08:39:26 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
10/12/2021 08:39:26 - INFO - transformers.tokenization_utils_base - loading file None
10/12/2021 08:39:26 - INFO - transformers.tokenization_utils_base - loading file None
10/12/2021 08:39:26 - INFO - transformers.tokenization_utils_base - loading file None
10/12/2021 08:39:26 - INFO - transformers.tokenization_utils_base - loading file None
10/12/2021 08:39:27 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_2/config.json
10/12/2021 08:39:27 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/12/2021 08:39:27 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_2/pytorch_model.bin
10/12/2021 08:39:32 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

10/12/2021 08:39:32 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_2.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
10/12/2021 08:39:32 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_2/pos/config.json
10/12/2021 08:39:32 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/12/2021 08:39:32 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_2/pos/pytorch_model.bin
10/12/2021 08:39:36 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BartForConditionalGeneration.

10/12/2021 08:39:36 - INFO - transformers.modeling_utils - All the weights of BartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_2/pos.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
10/12/2021 08:39:37 - INFO - __main__ - Starting training!
10/12/2021 08:46:02 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_0_200.txt
10/12/2021 08:46:05 - INFO - __main__ - 当前的评估值: bleu-4:61.02289314567565, ROUGE_L:79.04835960756807, CIDEr:5.240454359525634, Chrf:0.7917592836027276
10/12/2021 08:46:05 - INFO - __main__ - Step 200 Train loss 0.40 Learning rate 1.00e-06 BLEU 6102.29% on epoch=0
10/12/2021 08:46:05 - INFO - __main__ - 当前最好的：-1.00
10/12/2021 08:46:05 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/config.json
10/12/2021 08:46:09 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pytorch_model.bin
10/12/2021 08:46:09 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/pos/config.json
10/12/2021 08:46:13 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pos/pytorch_model.bin
10/12/2021 08:46:13 - INFO - __main__ - Saving model with best BLEU: -100.00% -> 6102.29% on epoch=0, global_step=200
10/12/2021 08:52:37 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_1_400.txt
10/12/2021 08:52:39 - INFO - __main__ - 当前的评估值: bleu-4:60.944435485836735, ROUGE_L:79.04552656891521, CIDEr:5.230762877017899, Chrf:0.7909857209060804
10/12/2021 08:52:39 - INFO - __main__ - Step 400 Train loss 0.40 Learning rate 2.00e-06 BLEU 6094.44% on epoch=1
10/12/2021 08:52:39 - INFO - __main__ - 当前最好的：61.02
10/12/2021 08:59:00 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_2_600.txt
10/12/2021 08:59:02 - INFO - __main__ - 当前的评估值: bleu-4:61.040855615171985, ROUGE_L:79.04427436693464, CIDEr:5.240602092589076, Chrf:0.7919916858534505
10/12/2021 08:59:02 - INFO - __main__ - Step 600 Train loss 0.40 Learning rate 3.00e-06 BLEU 6104.09% on epoch=1
10/12/2021 08:59:02 - INFO - __main__ - 当前最好的：61.02
10/12/2021 08:59:02 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_2/config.json
10/12/2021 08:59:06 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_2/pytorch_model.bin
10/12/2021 08:59:06 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_2/pos/config.json
10/12/2021 08:59:12 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_2/pos/pytorch_model.bin
10/12/2021 08:59:12 - INFO - __main__ - Saving model with best BLEU: 6102.29% -> 6104.09% on epoch=1, global_step=600
10/12/2021 09:05:34 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_3_800.txt
10/12/2021 09:05:37 - INFO - __main__ - 当前的评估值: bleu-4:61.298814756278006, ROUGE_L:79.23886812177972, CIDEr:5.283024084843541, Chrf:0.7932434112824213
10/12/2021 09:05:37 - INFO - __main__ - Step 800 Train loss 0.40 Learning rate 4.00e-06 BLEU 6129.88% on epoch=2
10/12/2021 09:05:37 - INFO - __main__ - 当前最好的：61.04
10/12/2021 09:05:37 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_3/config.json
10/12/2021 09:05:38 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_3/pytorch_model.bin
10/12/2021 09:05:38 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_3/pos/config.json
10/12/2021 09:05:39 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_3/pos/pytorch_model.bin
10/12/2021 09:05:39 - INFO - __main__ - Saving model with best BLEU: 6104.09% -> 6129.88% on epoch=2, global_step=800
11/02/2021 18:54:25 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=200, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-06, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_2', no_lr_decay=False, num_beams=5, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_2/pos', predict_batch_size=16, predict_file='data/webnlg_ground/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=34, train_file='data/webnlg_ground/train', wait_step=30, warmup_proportion=0.01, warmup_steps=2000, weight_decay=0.0)
11/02/2021 18:54:25 - INFO - __main__ - out/jointgt_bart_webnlg_local
11/02/2021 18:54:25 - INFO - __main__ - Using 3 gpus
11/02/2021 18:54:25 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
11/02/2021 18:54:25 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
11/02/2021 18:54:25 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
11/02/2021 18:54:25 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
11/02/2021 18:54:25 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
11/02/2021 18:54:25 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
11/02/2021 18:54:25 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
11/02/2021 18:54:25 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 18:54:25 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 18:54:25 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 18:54:25 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 18:54:26 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_2/config.json
11/02/2021 18:54:26 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/02/2021 18:54:26 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_2/pytorch_model.bin
11/02/2021 18:54:37 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

11/02/2021 18:54:37 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_2.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
11/02/2021 18:54:37 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_2/pos/config.json
11/02/2021 18:54:37 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/02/2021 18:54:37 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_2/pos/pytorch_model.bin
11/02/2021 18:54:44 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BartForConditionalGeneration.

11/02/2021 18:54:44 - INFO - transformers.modeling_utils - All the weights of BartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_2/pos.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
11/02/2021 18:54:55 - INFO - __main__ - Starting training!
11/02/2021 18:57:29 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=200, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-06, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_2', no_lr_decay=False, num_beams=5, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_2/pos', predict_batch_size=16, predict_file='data/webnlg_ground/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=36, train_file='data/webnlg_ground/train', wait_step=30, warmup_proportion=0.01, warmup_steps=1000, weight_decay=0.0)
11/02/2021 18:57:29 - INFO - __main__ - out/jointgt_bart_webnlg_local
11/02/2021 18:57:29 - INFO - __main__ - Using 3 gpus
11/02/2021 18:57:29 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
11/02/2021 18:57:29 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
11/02/2021 18:57:29 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
11/02/2021 18:57:29 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
11/02/2021 18:57:29 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
11/02/2021 18:57:29 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
11/02/2021 18:57:29 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
11/02/2021 18:57:29 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 18:57:29 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 18:57:29 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 18:57:29 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 18:57:29 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_2/config.json
11/02/2021 18:57:29 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/02/2021 18:57:29 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_2/pytorch_model.bin
11/02/2021 18:57:39 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

11/02/2021 18:57:39 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_2.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
11/02/2021 18:57:39 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_2/pos/config.json
11/02/2021 18:57:39 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/02/2021 18:57:39 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_2/pos/pytorch_model.bin
11/02/2021 18:57:44 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BartForConditionalGeneration.

11/02/2021 18:57:44 - INFO - transformers.modeling_utils - All the weights of BartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_2/pos.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
11/02/2021 18:57:49 - INFO - __main__ - Starting training!
11/02/2021 19:03:27 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_0_200.txt
11/02/2021 19:03:31 - INFO - __main__ - 当前的评估值: bleu-4:61.99655086142426, ROUGE_L:79.64859505065553, CIDEr:5.344099068529116, Chrf:0.7952311505906136
11/02/2021 19:03:31 - INFO - __main__ - Step 200 Train loss 0.31 Learning rate 2.00e-06 BLEU 6199.66% on epoch=0
11/02/2021 19:03:31 - INFO - __main__ - 当前最好的：-1.00
11/02/2021 19:03:31 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/config.json
11/02/2021 19:03:35 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pytorch_model.bin
11/02/2021 19:03:35 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/pos/config.json
11/02/2021 19:03:38 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pos/pytorch_model.bin
11/02/2021 19:03:38 - INFO - __main__ - Saving model with best BLEU: -100.00% -> 6199.66% on epoch=0, global_step=200
11/02/2021 19:09:04 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_1_400.txt
11/02/2021 19:09:08 - INFO - __main__ - 当前的评估值: bleu-4:61.822264190397604, ROUGE_L:79.60264550034084, CIDEr:5.328400142101446, Chrf:0.7941312210510396
11/02/2021 19:09:08 - INFO - __main__ - Step 400 Train loss 0.31 Learning rate 4.00e-06 BLEU 6182.23% on epoch=1
11/02/2021 19:09:08 - INFO - __main__ - 当前最好的：62.00
11/02/2021 19:10:09 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=200, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-06, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_0', no_lr_decay=False, num_beams=5, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_0/pos', predict_batch_size=16, predict_file='data/webnlg_ground/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=36, train_file='data/webnlg_ground/train', wait_step=30, warmup_proportion=0.01, warmup_steps=1000, weight_decay=0.0)
11/02/2021 19:10:09 - INFO - __main__ - out/jointgt_bart_webnlg_local
11/02/2021 19:10:09 - INFO - __main__ - Using 3 gpus
11/02/2021 19:10:09 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
11/02/2021 19:10:09 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
11/02/2021 19:10:09 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
11/02/2021 19:10:09 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
11/02/2021 19:10:09 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
11/02/2021 19:10:09 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
11/02/2021 19:10:09 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
11/02/2021 19:10:09 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 19:10:09 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 19:10:09 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 19:10:09 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 19:10:10 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_0/config.json
11/02/2021 19:10:10 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/02/2021 19:10:10 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_0/pytorch_model.bin
11/02/2021 19:10:19 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

11/02/2021 19:10:19 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_0.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
11/02/2021 19:10:19 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_0/pos/config.json
11/02/2021 19:10:19 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/02/2021 19:10:19 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_0/pos/pytorch_model.bin
11/02/2021 19:10:23 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BartForConditionalGeneration.

11/02/2021 19:10:23 - INFO - transformers.modeling_utils - All the weights of BartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_0/pos.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
11/02/2021 19:10:28 - INFO - __main__ - Starting training!
11/02/2021 19:16:10 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_0_200.txt
11/02/2021 19:16:13 - INFO - __main__ - 当前的评估值: bleu-4:61.918098795373176, ROUGE_L:79.59903408242256, CIDEr:5.332307883938639, Chrf:0.7948401729724959
11/02/2021 19:16:13 - INFO - __main__ - Step 200 Train loss 0.30 Learning rate 2.00e-07 BLEU 6191.81% on epoch=0
11/02/2021 19:16:13 - INFO - __main__ - 当前最好的：-1.00
11/02/2021 19:16:13 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/config.json
11/02/2021 19:16:17 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pytorch_model.bin
11/02/2021 19:16:17 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/pos/config.json
11/02/2021 19:16:20 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pos/pytorch_model.bin
11/02/2021 19:16:20 - INFO - __main__ - Saving model with best BLEU: -100.00% -> 6191.81% on epoch=0, global_step=200
11/02/2021 19:24:05 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_1_400.txt
11/02/2021 19:24:09 - INFO - __main__ - 当前的评估值: bleu-4:62.039784553352426, ROUGE_L:79.72339506480114, CIDEr:5.343762815155736, Chrf:0.7947129172679157
11/02/2021 19:24:09 - INFO - __main__ - Step 400 Train loss 0.31 Learning rate 4.00e-07 BLEU 6203.98% on epoch=1
11/02/2021 19:24:09 - INFO - __main__ - 当前最好的：61.92
11/02/2021 19:24:09 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_1/config.json
11/02/2021 19:24:11 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_1/pytorch_model.bin
11/02/2021 19:24:11 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_1/pos/config.json
11/02/2021 19:24:12 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_1/pos/pytorch_model.bin
11/02/2021 19:24:12 - INFO - __main__ - Saving model with best BLEU: 6191.81% -> 6203.98% on epoch=1, global_step=400
11/02/2021 19:35:12 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_2_600.txt
11/02/2021 19:35:15 - INFO - __main__ - 当前的评估值: bleu-4:62.17963391555137, ROUGE_L:79.7782635282657, CIDEr:5.357083207889216, Chrf:0.7963414417715611
11/02/2021 19:35:15 - INFO - __main__ - Step 600 Train loss 0.30 Learning rate 6.00e-07 BLEU 6217.96% on epoch=1
11/02/2021 19:35:15 - INFO - __main__ - 当前最好的：62.04
11/02/2021 19:35:15 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_2/config.json
11/02/2021 19:35:19 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_2/pytorch_model.bin
11/02/2021 19:35:19 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_2/pos/config.json
11/02/2021 19:35:23 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_2/pos/pytorch_model.bin
11/02/2021 19:35:23 - INFO - __main__ - Saving model with best BLEU: 6203.98% -> 6217.96% on epoch=1, global_step=600
11/02/2021 19:46:20 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_3_800.txt
11/02/2021 19:46:24 - INFO - __main__ - 当前的评估值: bleu-4:62.040422096724704, ROUGE_L:79.75569128568554, CIDEr:5.346531209106627, Chrf:0.794821757084221
11/02/2021 19:46:24 - INFO - __main__ - Step 800 Train loss 0.31 Learning rate 8.00e-07 BLEU 6204.04% on epoch=2
11/02/2021 19:46:24 - INFO - __main__ - 当前最好的：62.18
11/02/2021 19:57:21 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_4_1000.txt
11/02/2021 19:57:25 - INFO - __main__ - 当前的评估值: bleu-4:62.397431708494736, ROUGE_L:79.93531983089032, CIDEr:5.374842817994172, Chrf:0.7970309891106836
11/02/2021 19:57:25 - INFO - __main__ - Step 1000 Train loss 0.30 Learning rate 1.00e-06 BLEU 6239.74% on epoch=3
11/02/2021 19:57:25 - INFO - __main__ - 当前最好的：62.18
11/02/2021 19:57:25 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_4/config.json
11/02/2021 19:57:27 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_4/pytorch_model.bin
11/02/2021 19:57:27 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_4/pos/config.json
11/02/2021 19:57:28 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_4/pos/pytorch_model.bin
11/02/2021 19:57:28 - INFO - __main__ - Saving model with best BLEU: 6217.96% -> 6239.74% on epoch=3, global_step=1000
11/02/2021 20:08:19 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_5_1200.txt
11/02/2021 20:08:23 - INFO - __main__ - 当前的评估值: bleu-4:62.128235483608975, ROUGE_L:79.88549147340316, CIDEr:5.370042950474291, Chrf:0.7953883911827111
11/02/2021 20:08:23 - INFO - __main__ - Step 1200 Train loss 0.30 Learning rate 9.93e-07 BLEU 6212.82% on epoch=3
11/02/2021 20:08:23 - INFO - __main__ - 当前最好的：62.40
11/02/2021 20:19:25 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_6_1400.txt
11/02/2021 20:19:29 - INFO - __main__ - 当前的评估值: bleu-4:61.95820701654182, ROUGE_L:79.73475081724294, CIDEr:5.338148320293896, Chrf:0.7941509974340568
11/02/2021 20:19:29 - INFO - __main__ - Step 1400 Train loss 0.30 Learning rate 9.87e-07 BLEU 6195.82% on epoch=4
11/02/2021 20:19:29 - INFO - __main__ - 当前最好的：62.40
11/02/2021 20:26:45 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_7_1600.txt
11/02/2021 20:26:50 - INFO - __main__ - 当前的评估值: bleu-4:62.15558675470814, ROUGE_L:79.89473791017882, CIDEr:5.372408625838909, Chrf:0.7958195343543091
11/02/2021 20:26:50 - INFO - __main__ - Step 1600 Train loss 0.29 Learning rate 9.80e-07 BLEU 6215.56% on epoch=5
11/02/2021 20:26:50 - INFO - __main__ - 当前最好的：62.40
11/02/2021 20:35:27 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_8_1800.txt
11/02/2021 20:35:32 - INFO - __main__ - 当前的评估值: bleu-4:62.35300828851842, ROUGE_L:79.92582840351812, CIDEr:5.3936176579206005, Chrf:0.796875947339708
11/02/2021 20:35:32 - INFO - __main__ - Step 1800 Train loss 0.29 Learning rate 9.74e-07 BLEU 6235.30% on epoch=5
11/02/2021 20:35:32 - INFO - __main__ - 当前最好的：62.40
11/02/2021 20:43:53 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_9_2000.txt
11/02/2021 20:43:58 - INFO - __main__ - 当前的评估值: bleu-4:62.15731362037912, ROUGE_L:79.83183496875797, CIDEr:5.361847168727668, Chrf:0.7956863871033376
11/02/2021 20:43:58 - INFO - __main__ - Step 2000 Train loss 0.29 Learning rate 9.67e-07 BLEU 6215.73% on epoch=6
11/02/2021 20:43:58 - INFO - __main__ - 当前最好的：62.40
11/02/2021 20:52:33 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_10_2200.txt
11/02/2021 20:52:37 - INFO - __main__ - 当前的评估值: bleu-4:62.15895261413213, ROUGE_L:79.82703665876811, CIDEr:5.372328617786036, Chrf:0.7964904810488977
11/02/2021 20:52:37 - INFO - __main__ - Step 2200 Train loss 0.29 Learning rate 9.60e-07 BLEU 6215.90% on epoch=7
11/02/2021 20:52:37 - INFO - __main__ - 当前最好的：62.40
11/02/2021 21:00:52 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_11_2400.txt
11/02/2021 21:00:56 - INFO - __main__ - 当前的评估值: bleu-4:61.9920742759555, ROUGE_L:79.71310483481459, CIDEr:5.342274121764229, Chrf:0.7944044422998513
11/02/2021 21:00:57 - INFO - __main__ - Step 2400 Train loss 0.29 Learning rate 9.54e-07 BLEU 6199.21% on epoch=7
11/02/2021 21:00:57 - INFO - __main__ - 当前最好的：62.40
11/02/2021 21:09:11 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_12_2600.txt
11/02/2021 21:09:14 - INFO - __main__ - 当前的评估值: bleu-4:61.88955798576637, ROUGE_L:79.65413409916646, CIDEr:5.325590219873192, Chrf:0.7942528141135771
11/02/2021 21:09:14 - INFO - __main__ - Step 2600 Train loss 0.28 Learning rate 9.47e-07 BLEU 6188.96% on epoch=8
11/02/2021 21:09:14 - INFO - __main__ - 当前最好的：62.40
11/02/2021 21:14:40 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_13_2800.txt
11/02/2021 21:14:43 - INFO - __main__ - 当前的评估值: bleu-4:61.90527623109726, ROUGE_L:79.67031792116939, CIDEr:5.3311079052746315, Chrf:0.7942066022378773
11/02/2021 21:14:43 - INFO - __main__ - Step 2800 Train loss 0.29 Learning rate 9.41e-07 BLEU 6190.53% on epoch=8
11/02/2021 21:14:43 - INFO - __main__ - 当前最好的：62.40
11/02/2021 21:20:09 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_14_3000.txt
11/02/2021 21:20:12 - INFO - __main__ - 当前的评估值: bleu-4:62.106124899408336, ROUGE_L:79.79148678800101, CIDEr:5.354890378296062, Chrf:0.7949233205854326
11/02/2021 21:20:12 - INFO - __main__ - Step 3000 Train loss 0.29 Learning rate 9.34e-07 BLEU 6210.61% on epoch=9
11/02/2021 21:20:12 - INFO - __main__ - 当前最好的：62.40
11/02/2021 21:25:43 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=200, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-05, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_4_best', no_lr_decay=False, num_beams=5, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_4_best/pos', predict_batch_size=16, predict_file='data/webnlg_ground/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=36, train_file='data/webnlg_ground/train', wait_step=30, warmup_proportion=0.01, warmup_steps=1000, weight_decay=0.0)
11/02/2021 21:25:43 - INFO - __main__ - out/jointgt_bart_webnlg_local
11/02/2021 21:25:43 - INFO - __main__ - Using 3 gpus
11/02/2021 21:25:43 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
11/02/2021 21:25:43 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
11/02/2021 21:25:43 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
11/02/2021 21:25:43 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
11/02/2021 21:25:43 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
11/02/2021 21:25:43 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
11/02/2021 21:25:43 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
11/02/2021 21:25:43 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:25:43 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:25:43 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:25:43 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:25:44 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_4_best/config.json
11/02/2021 21:25:44 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/02/2021 21:25:44 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_4_best/pytorch_model.bin
11/02/2021 21:25:52 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

11/02/2021 21:25:52 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_4_best.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
11/02/2021 21:25:52 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_4_best/pos/config.json
11/02/2021 21:25:52 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/02/2021 21:25:52 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_4_best/pos/pytorch_model.bin
11/02/2021 21:25:57 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BartForConditionalGeneration.

11/02/2021 21:25:57 - INFO - transformers.modeling_utils - All the weights of BartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_4_best/pos.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
11/02/2021 21:26:02 - INFO - __main__ - Starting training!
11/02/2021 21:26:29 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=200, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-05, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_4_best', no_lr_decay=False, num_beams=5, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_4_best/pos', predict_batch_size=16, predict_file='data/webnlg_ground/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=36, train_file='data/webnlg_ground/train', wait_step=30, warmup_proportion=0.01, warmup_steps=1000, weight_decay=0.0)
11/02/2021 21:26:29 - INFO - __main__ - out/jointgt_bart_webnlg_local
11/02/2021 21:26:29 - INFO - __main__ - Using 3 gpus
11/02/2021 21:26:29 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
11/02/2021 21:26:29 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
11/02/2021 21:26:29 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
11/02/2021 21:26:29 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
11/02/2021 21:26:29 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
11/02/2021 21:26:29 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
11/02/2021 21:26:29 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
11/02/2021 21:26:29 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:26:29 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:26:29 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:26:29 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:26:30 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_4_best/config.json
11/02/2021 21:26:30 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/02/2021 21:26:30 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_4_best/pytorch_model.bin
11/02/2021 21:26:38 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

11/02/2021 21:26:38 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_4_best.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
11/02/2021 21:26:39 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_4_best/pos/config.json
11/02/2021 21:26:39 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/02/2021 21:26:39 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_4_best/pos/pytorch_model.bin
11/02/2021 21:26:43 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BartForConditionalGeneration.

11/02/2021 21:26:43 - INFO - transformers.modeling_utils - All the weights of BartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_4_best/pos.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
11/02/2021 21:26:48 - INFO - __main__ - Starting training!
11/02/2021 21:34:54 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=200, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-05, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_4_best', no_lr_decay=False, num_beams=5, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_4_best/pos', predict_batch_size=16, predict_file='data/webnlg_ground/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=36, train_file='data/webnlg_ground/train', wait_step=30, warmup_proportion=0.01, warmup_steps=1000, weight_decay=0.0)
11/02/2021 21:34:54 - INFO - __main__ - out/jointgt_bart_webnlg_local
11/02/2021 21:34:54 - INFO - __main__ - Using 3 gpus
11/02/2021 21:34:54 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
11/02/2021 21:34:54 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
11/02/2021 21:34:54 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
11/02/2021 21:34:54 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
11/02/2021 21:34:54 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
11/02/2021 21:34:54 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
11/02/2021 21:34:54 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
11/02/2021 21:34:54 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:34:54 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:34:54 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:34:54 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:34:55 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_4_best/config.json
11/02/2021 21:34:55 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/02/2021 21:34:55 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_4_best/pytorch_model.bin
11/02/2021 21:35:03 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

11/02/2021 21:35:03 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_4_best.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
11/02/2021 21:35:03 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_4_best/pos/config.json
11/02/2021 21:35:03 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/02/2021 21:35:03 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_4_best/pos/pytorch_model.bin
11/02/2021 21:35:07 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BartForConditionalGeneration.

11/02/2021 21:35:07 - INFO - transformers.modeling_utils - All the weights of BartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_4_best/pos.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
11/02/2021 21:35:13 - INFO - __main__ - Starting training!
11/02/2021 21:40:54 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_0_200.txt
11/02/2021 21:40:58 - INFO - __main__ - 当前的评估值: bleu-4:62.0631389276149, ROUGE_L:79.69141360868156, CIDEr:5.342503937457105, Chrf:0.7952496201206549 chrf2:0.7985515548997233
11/02/2021 21:40:58 - INFO - __main__ - Step 200 Train loss 0.29 Learning rate 2.00e-07 BLEU 6206.31% on epoch=0
11/02/2021 21:40:58 - INFO - __main__ - 当前最好的：-1.00
11/02/2021 21:40:58 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/config.json
11/02/2021 21:41:02 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pytorch_model.bin
11/02/2021 21:41:02 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/pos/config.json
11/02/2021 21:41:05 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pos/pytorch_model.bin
11/02/2021 21:41:05 - INFO - __main__ - Saving model with best BLEU: -100.00% -> 6206.31% on epoch=0, global_step=200
11/02/2021 21:46:35 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_1_400.txt
11/02/2021 21:46:39 - INFO - __main__ - 当前的评估值: bleu-4:62.05793076796263, ROUGE_L:79.75238024616174, CIDEr:5.346659114620143, Chrf:0.7945156138449518 chrf2:0.7981271246547057
11/02/2021 21:46:39 - INFO - __main__ - Step 400 Train loss 0.29 Learning rate 4.00e-07 BLEU 6205.79% on epoch=1
11/02/2021 21:46:39 - INFO - __main__ - 当前最好的：62.06
11/02/2021 21:50:22 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=200, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-05, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_4_best', no_lr_decay=False, num_beams=8, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_4_best/pos', predict_batch_size=16, predict_file='data/webnlg_ground/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=36, train_file='data/webnlg_ground/train', wait_step=30, warmup_proportion=0.01, warmup_steps=1000, weight_decay=0.0)
11/02/2021 21:50:22 - INFO - __main__ - out/jointgt_bart_webnlg_local
11/02/2021 21:50:22 - INFO - __main__ - Using 3 gpus
11/02/2021 21:50:22 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
11/02/2021 21:50:22 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
11/02/2021 21:50:22 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
11/02/2021 21:50:22 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
11/02/2021 21:50:22 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
11/02/2021 21:50:22 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
11/02/2021 21:50:22 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
11/02/2021 21:50:22 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:50:22 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:50:22 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:50:22 - INFO - transformers.tokenization_utils_base - loading file None
11/02/2021 21:50:22 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_4_best/config.json
11/02/2021 21:50:22 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/02/2021 21:50:22 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_4_best/pytorch_model.bin
11/02/2021 21:50:31 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

11/02/2021 21:50:31 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_4_best.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
11/02/2021 21:50:32 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_4_best/pos/config.json
11/02/2021 21:50:32 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/02/2021 21:50:32 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_4_best/pos/pytorch_model.bin
11/02/2021 21:50:36 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BartForConditionalGeneration.

11/02/2021 21:50:36 - INFO - transformers.modeling_utils - All the weights of BartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_4_best/pos.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
11/02/2021 21:50:41 - INFO - __main__ - Starting training!
11/02/2021 21:57:08 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_0_200.txt
11/02/2021 21:57:12 - INFO - __main__ - 当前的评估值: bleu-4:60.97887995263635, ROUGE_L:79.0170840126256, CIDEr:5.234004620218062, Chrf:0.7914164962934347 chrf2:0.7946618249408064
11/02/2021 21:57:12 - INFO - __main__ - Step 200 Train loss 0.29 Learning rate 2.00e-07 BLEU 6097.89% on epoch=0
11/02/2021 21:57:12 - INFO - __main__ - 当前最好的：-1.00
11/02/2021 21:57:12 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/config.json
11/02/2021 21:57:16 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pytorch_model.bin
11/02/2021 21:57:16 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/pos/config.json
11/02/2021 21:57:19 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pos/pytorch_model.bin
11/02/2021 21:57:19 - INFO - __main__ - Saving model with best BLEU: -100.00% -> 6097.89% on epoch=0, global_step=200
11/02/2021 22:03:37 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_1_400.txt
11/02/2021 22:03:41 - INFO - __main__ - 当前的评估值: bleu-4:60.896172926699876, ROUGE_L:79.0182433548577, CIDEr:5.212583765173143, Chrf:0.7904349865132684 chrf2:0.7939895615209676
11/02/2021 22:03:41 - INFO - __main__ - Step 400 Train loss 0.29 Learning rate 4.00e-07 BLEU 6089.62% on epoch=1
11/02/2021 22:03:41 - INFO - __main__ - 当前最好的：60.98
11/02/2021 22:09:52 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_2_600.txt
11/02/2021 22:09:55 - INFO - __main__ - 当前的评估值: bleu-4:61.14086910334724, ROUGE_L:79.05717671128542, CIDEr:5.247783098409896, Chrf:0.7914875121144909 chrf2:0.79499912546784
11/02/2021 22:09:55 - INFO - __main__ - Step 600 Train loss 0.29 Learning rate 6.00e-07 BLEU 6114.09% on epoch=1
11/02/2021 22:09:55 - INFO - __main__ - 当前最好的：60.98
11/02/2021 22:09:55 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_2/config.json
11/02/2021 22:09:59 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_2/pytorch_model.bin
11/02/2021 22:09:59 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_2/pos/config.json
11/02/2021 22:10:02 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_2/pos/pytorch_model.bin
11/02/2021 22:10:02 - INFO - __main__ - Saving model with best BLEU: 6097.89% -> 6114.09% on epoch=1, global_step=600
11/02/2021 22:16:18 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_3_800.txt
11/02/2021 22:16:22 - INFO - __main__ - 当前的评估值: bleu-4:60.95134609392431, ROUGE_L:78.9810356196092, CIDEr:5.231479623557494, Chrf:0.7906322313992692 chrf2:0.7944077429389743
11/02/2021 22:16:22 - INFO - __main__ - Step 800 Train loss 0.29 Learning rate 8.00e-07 BLEU 6095.13% on epoch=2
11/02/2021 22:16:22 - INFO - __main__ - 当前最好的：61.14
11/02/2021 22:22:42 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_4_1000.txt
11/02/2021 22:22:46 - INFO - __main__ - 当前的评估值: bleu-4:60.99850996970779, ROUGE_L:78.97911271035406, CIDEr:5.233395496307973, Chrf:0.7911798923546243 chrf2:0.7946469631536143
11/02/2021 22:22:46 - INFO - __main__ - Step 1000 Train loss 0.28 Learning rate 1.00e-06 BLEU 6099.85% on epoch=3
11/02/2021 22:22:46 - INFO - __main__ - 当前最好的：61.14
11/02/2021 22:28:58 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_5_1200.txt
11/02/2021 22:29:01 - INFO - __main__ - 当前的评估值: bleu-4:61.24552097763753, ROUGE_L:79.22488651810818, CIDEr:5.259753844054234, Chrf:0.7914969672520216 chrf2:0.7952756928554119
11/02/2021 22:29:01 - INFO - __main__ - Step 1200 Train loss 0.29 Learning rate 9.93e-07 BLEU 6124.55% on epoch=3
11/02/2021 22:29:01 - INFO - __main__ - 当前最好的：61.14
11/02/2021 22:29:01 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_5/config.json
11/02/2021 22:29:03 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_5/pytorch_model.bin
11/02/2021 22:29:03 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_5/pos/config.json
11/02/2021 22:29:04 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_5/pos/pytorch_model.bin
11/02/2021 22:29:04 - INFO - __main__ - Saving model with best BLEU: 6114.09% -> 6124.55% on epoch=3, global_step=1200
11/02/2021 22:35:08 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_6_1400.txt
11/02/2021 22:35:11 - INFO - __main__ - 当前的评估值: bleu-4:60.78502086349681, ROUGE_L:78.91211223323383, CIDEr:5.212988659573092, Chrf:0.78931532964034 chrf2:0.7930227989000913
11/02/2021 22:35:11 - INFO - __main__ - Step 1400 Train loss 0.30 Learning rate 9.87e-07 BLEU 6078.50% on epoch=4
11/02/2021 22:35:11 - INFO - __main__ - 当前最好的：61.25
11/02/2021 22:41:26 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_7_1600.txt
11/02/2021 22:41:29 - INFO - __main__ - 当前的评估值: bleu-4:61.079535394827374, ROUGE_L:79.07236479164472, CIDEr:5.247249334014223, Chrf:0.7912747940392266 chrf2:0.7947928434460182
11/02/2021 22:41:29 - INFO - __main__ - Step 1600 Train loss 0.29 Learning rate 9.80e-07 BLEU 6107.95% on epoch=5
11/02/2021 22:41:29 - INFO - __main__ - 当前最好的：61.25
11/02/2021 22:47:47 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_8_1800.txt
11/02/2021 22:47:51 - INFO - __main__ - 当前的评估值: bleu-4:61.270586751100566, ROUGE_L:79.10770843428551, CIDEr:5.260852289029694, Chrf:0.7920372767219708 chrf2:0.7954250729652813
11/02/2021 22:47:51 - INFO - __main__ - Step 1800 Train loss 0.29 Learning rate 9.74e-07 BLEU 6127.06% on epoch=5
11/02/2021 22:47:51 - INFO - __main__ - 当前最好的：61.25
11/02/2021 22:47:51 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_8/config.json
11/02/2021 22:47:52 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_8/pytorch_model.bin
11/02/2021 22:47:52 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_8/pos/config.json
11/02/2021 22:47:53 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_8/pos/pytorch_model.bin
11/02/2021 22:47:53 - INFO - __main__ - Saving model with best BLEU: 6124.55% -> 6127.06% on epoch=5, global_step=1800
11/02/2021 22:54:54 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_9_2000.txt
11/02/2021 22:54:58 - INFO - __main__ - 当前的评估值: bleu-4:61.06748071032242, ROUGE_L:79.03077194947056, CIDEr:5.233265314563544, Chrf:0.7908386191540313 chrf2:0.7943143245643632
11/02/2021 22:54:58 - INFO - __main__ - Step 2000 Train loss 0.29 Learning rate 9.67e-07 BLEU 6106.75% on epoch=6
11/02/2021 22:54:58 - INFO - __main__ - 当前最好的：61.27
11/02/2021 23:02:00 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_10_2200.txt
11/02/2021 23:02:04 - INFO - __main__ - 当前的评估值: bleu-4:61.11192804933743, ROUGE_L:79.08463681401908, CIDEr:5.248131636978385, Chrf:0.7907554339221281 chrf2:0.7945241156554738
11/02/2021 23:02:04 - INFO - __main__ - Step 2200 Train loss 0.29 Learning rate 9.60e-07 BLEU 6111.19% on epoch=7
11/02/2021 23:02:04 - INFO - __main__ - 当前最好的：61.27
11/02/2021 23:09:04 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_11_2400.txt
11/02/2021 23:09:07 - INFO - __main__ - 当前的评估值: bleu-4:61.19377526716639, ROUGE_L:79.17716699324552, CIDEr:5.250510364435037, Chrf:0.7907028451947701 chrf2:0.7945327056842602
11/02/2021 23:09:07 - INFO - __main__ - Step 2400 Train loss 0.28 Learning rate 9.54e-07 BLEU 6119.38% on epoch=7
11/02/2021 23:09:07 - INFO - __main__ - 当前最好的：61.27
11/02/2021 23:16:04 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_12_2600.txt
11/02/2021 23:16:08 - INFO - __main__ - 当前的评估值: bleu-4:61.05952804619673, ROUGE_L:79.08114774445701, CIDEr:5.243253034628579, Chrf:0.7910969676858463 chrf2:0.7946529189944105
11/02/2021 23:16:08 - INFO - __main__ - Step 2600 Train loss 0.28 Learning rate 9.47e-07 BLEU 6105.95% on epoch=8
11/02/2021 23:16:08 - INFO - __main__ - 当前最好的：61.27
11/02/2021 23:23:09 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_13_2800.txt
11/02/2021 23:23:13 - INFO - __main__ - 当前的评估值: bleu-4:61.20906146927323, ROUGE_L:79.18003418678906, CIDEr:5.248183064534239, Chrf:0.7915070775673811 chrf2:0.795133190277348
11/02/2021 23:23:13 - INFO - __main__ - Step 2800 Train loss 0.28 Learning rate 9.41e-07 BLEU 6120.91% on epoch=8
11/02/2021 23:23:13 - INFO - __main__ - 当前最好的：61.27
11/02/2021 23:30:09 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_14_3000.txt
11/02/2021 23:30:13 - INFO - __main__ - 当前的评估值: bleu-4:61.30284415772177, ROUGE_L:79.32770651077391, CIDEr:5.2603645860709065, Chrf:0.7913649180544424 chrf2:0.7948014932901823
11/02/2021 23:30:13 - INFO - __main__ - Step 3000 Train loss 0.28 Learning rate 9.34e-07 BLEU 6130.28% on epoch=9
11/02/2021 23:30:13 - INFO - __main__ - 当前最好的：61.27
11/02/2021 23:30:13 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_14/config.json
11/02/2021 23:30:14 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_14/pytorch_model.bin
11/02/2021 23:30:14 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_14/pos/config.json
11/02/2021 23:30:15 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_14/pos/pytorch_model.bin
11/02/2021 23:30:15 - INFO - __main__ - Saving model with best BLEU: 6127.06% -> 6130.28% on epoch=9, global_step=3000
11/02/2021 23:37:13 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_15_3200.txt
11/02/2021 23:37:16 - INFO - __main__ - 当前的评估值: bleu-4:61.16867898036119, ROUGE_L:79.15215092563741, CIDEr:5.253460518249596, Chrf:0.7911932596473286 chrf2:0.7946435239797507
11/02/2021 23:37:16 - INFO - __main__ - Step 3200 Train loss 0.28 Learning rate 9.27e-07 BLEU 6116.87% on epoch=10
11/02/2021 23:37:16 - INFO - __main__ - 当前最好的：61.30
11/02/2021 23:44:18 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_16_3400.txt
11/02/2021 23:44:22 - INFO - __main__ - 当前的评估值: bleu-4:60.95882255007293, ROUGE_L:79.08309409065278, CIDEr:5.235140254090705, Chrf:0.78944022756381 chrf2:0.7932848025188848
11/02/2021 23:44:22 - INFO - __main__ - Step 3400 Train loss 0.28 Learning rate 9.21e-07 BLEU 6095.88% on epoch=10
11/02/2021 23:44:22 - INFO - __main__ - 当前最好的：61.30
11/02/2021 23:51:17 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_17_3600.txt
11/02/2021 23:51:21 - INFO - __main__ - 当前的评估值: bleu-4:61.361142903193574, ROUGE_L:79.2862081979467, CIDEr:5.263081049371907, Chrf:0.7914851412589365 chrf2:0.7952303816035793
11/02/2021 23:51:21 - INFO - __main__ - Step 3600 Train loss 0.27 Learning rate 9.14e-07 BLEU 6136.11% on epoch=11
11/02/2021 23:51:21 - INFO - __main__ - 当前最好的：61.30
11/02/2021 23:51:21 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_17/config.json
11/02/2021 23:51:23 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_17/pytorch_model.bin
11/02/2021 23:51:23 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_17/pos/config.json
11/02/2021 23:51:24 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_17/pos/pytorch_model.bin
11/02/2021 23:51:24 - INFO - __main__ - Saving model with best BLEU: 6130.28% -> 6136.11% on epoch=11, global_step=3600
11/02/2021 23:58:19 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_18_3800.txt
11/02/2021 23:58:23 - INFO - __main__ - 当前的评估值: bleu-4:61.61616541728138, ROUGE_L:79.56223675460424, CIDEr:5.30495526220429, Chrf:0.7929389697990323 chrf2:0.7966684715501561
11/02/2021 23:58:23 - INFO - __main__ - Step 3800 Train loss 0.28 Learning rate 9.08e-07 BLEU 6161.62% on epoch=12
11/02/2021 23:58:23 - INFO - __main__ - 当前最好的：61.36
11/02/2021 23:58:23 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_18/config.json
11/02/2021 23:58:24 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_18/pytorch_model.bin
11/02/2021 23:58:24 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_18/pos/config.json
11/02/2021 23:58:25 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_18/pos/pytorch_model.bin
11/02/2021 23:58:25 - INFO - __main__ - Saving model with best BLEU: 6136.11% -> 6161.62% on epoch=12, global_step=3800
11/03/2021 00:05:22 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_19_4000.txt
11/03/2021 00:05:26 - INFO - __main__ - 当前的评估值: bleu-4:61.14980846160272, ROUGE_L:79.19683490286535, CIDEr:5.248408847092264, Chrf:0.7909680191715014 chrf2:0.7944868589120733
11/03/2021 00:05:26 - INFO - __main__ - Step 4000 Train loss 0.28 Learning rate 9.01e-07 BLEU 6114.98% on epoch=12
11/03/2021 00:05:26 - INFO - __main__ - 当前最好的：61.62
11/03/2021 00:12:10 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_20_4200.txt
11/03/2021 00:12:14 - INFO - __main__ - 当前的评估值: bleu-4:61.09633105033415, ROUGE_L:79.21024956367735, CIDEr:5.2390825664679035, Chrf:0.7908997204877515 chrf2:0.7941887880061689
11/03/2021 00:12:14 - INFO - __main__ - Step 4200 Train loss 0.28 Learning rate 8.94e-07 BLEU 6109.63% on epoch=13
11/03/2021 00:12:14 - INFO - __main__ - 当前最好的：61.62
11/03/2021 00:18:28 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_21_4400.txt
11/03/2021 00:18:31 - INFO - __main__ - 当前的评估值: bleu-4:60.95713652249628, ROUGE_L:79.0395331757303, CIDEr:5.227171573480576, Chrf:0.7895996323432191 chrf2:0.7929145729597687
11/03/2021 00:18:31 - INFO - __main__ - Step 4400 Train loss 0.27 Learning rate 8.88e-07 BLEU 6095.71% on epoch=14
11/03/2021 00:18:31 - INFO - __main__ - 当前最好的：61.62
11/03/2021 00:24:42 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_22_4600.txt
11/03/2021 00:24:45 - INFO - __main__ - 当前的评估值: bleu-4:60.801540255379194, ROUGE_L:79.00120725817918, CIDEr:5.218934542867423, Chrf:0.7890990836708187 chrf2:0.7927718167594395
11/03/2021 00:24:45 - INFO - __main__ - Step 4600 Train loss 0.27 Learning rate 8.81e-07 BLEU 6080.15% on epoch=14
11/03/2021 00:24:45 - INFO - __main__ - 当前最好的：61.62
11/03/2021 00:30:55 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_23_4800.txt
11/03/2021 00:30:59 - INFO - __main__ - 当前的评估值: bleu-4:60.80117332088324, ROUGE_L:78.88674017471591, CIDEr:5.211938518041337, Chrf:0.7887341809146704 chrf2:0.792181147777979
11/03/2021 00:30:59 - INFO - __main__ - Step 4800 Train loss 0.27 Learning rate 8.75e-07 BLEU 6080.12% on epoch=15
11/03/2021 00:30:59 - INFO - __main__ - 当前最好的：61.62
11/03/2021 00:37:12 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_24_5000.txt
11/03/2021 00:37:16 - INFO - __main__ - 当前的评估值: bleu-4:60.95773746682956, ROUGE_L:79.10998682553912, CIDEr:5.2362567311369075, Chrf:0.789821924695796 chrf2:0.7932881105821379
11/03/2021 00:37:16 - INFO - __main__ - Step 5000 Train loss 0.27 Learning rate 8.68e-07 BLEU 6095.77% on epoch=15
11/03/2021 00:37:16 - INFO - __main__ - 当前最好的：61.62
11/03/2021 00:43:26 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_25_5200.txt
11/03/2021 00:43:30 - INFO - __main__ - 当前的评估值: bleu-4:60.947619947799666, ROUGE_L:79.1470983514682, CIDEr:5.227264418795706, Chrf:0.7894123813419398 chrf2:0.7930272481670719
11/03/2021 00:43:30 - INFO - __main__ - Step 5200 Train loss 0.27 Learning rate 8.61e-07 BLEU 6094.76% on epoch=16
11/03/2021 00:43:30 - INFO - __main__ - 当前最好的：61.62
11/03/2021 00:49:46 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_26_5400.txt
11/03/2021 00:49:49 - INFO - __main__ - 当前的评估值: bleu-4:60.462109122369846, ROUGE_L:78.73341864648617, CIDEr:5.179304200391669, Chrf:0.7864528883774954 chrf2:0.7901359209229699
11/03/2021 00:49:49 - INFO - __main__ - Step 5400 Train loss 0.27 Learning rate 8.55e-07 BLEU 6046.21% on epoch=17
11/03/2021 00:49:49 - INFO - __main__ - 当前最好的：61.62
11/03/2021 00:55:56 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_27_5600.txt
11/03/2021 00:55:59 - INFO - __main__ - 当前的评估值: bleu-4:61.061787963181935, ROUGE_L:79.28432187012989, CIDEr:5.249107926703184, Chrf:0.789451858801954 chrf2:0.7931606694534973
11/03/2021 00:55:59 - INFO - __main__ - Step 5600 Train loss 0.27 Learning rate 8.48e-07 BLEU 6106.18% on epoch=17
11/03/2021 00:55:59 - INFO - __main__ - 当前最好的：61.62
11/03/2021 01:02:12 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_28_5800.txt
11/03/2021 01:02:15 - INFO - __main__ - 当前的评估值: bleu-4:60.85945989041104, ROUGE_L:79.29382938670716, CIDEr:5.249030888674283, Chrf:0.7880062101452068 chrf2:0.7919556756345258
11/03/2021 01:02:15 - INFO - __main__ - Step 5800 Train loss 0.27 Learning rate 8.42e-07 BLEU 6085.95% on epoch=18
11/03/2021 01:02:15 - INFO - __main__ - 当前最好的：61.62
11/03/2021 01:08:29 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_29_6000.txt
11/03/2021 01:08:33 - INFO - __main__ - 当前的评估值: bleu-4:60.825314692846575, ROUGE_L:79.1182688785342, CIDEr:5.233890262807091, Chrf:0.7886124155893282 chrf2:0.7925209503250231
11/03/2021 01:08:33 - INFO - __main__ - Step 6000 Train loss 0.27 Learning rate 8.35e-07 BLEU 6082.53% on epoch=19
11/03/2021 01:08:33 - INFO - __main__ - 当前最好的：61.62
11/03/2021 01:14:44 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_30_6200.txt
11/03/2021 01:14:47 - INFO - __main__ - 当前的评估值: bleu-4:60.449251369686394, ROUGE_L:78.84300995382971, CIDEr:5.200380864519509, Chrf:0.7861574136781997 chrf2:0.7901299320563855
11/03/2021 01:14:47 - INFO - __main__ - Step 6200 Train loss 0.27 Learning rate 8.28e-07 BLEU 6044.93% on epoch=19
11/03/2021 01:14:47 - INFO - __main__ - 当前最好的：61.62
11/03/2021 01:21:08 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_31_6400.txt
11/03/2021 01:21:12 - INFO - __main__ - 当前的评估值: bleu-4:60.627078224877074, ROUGE_L:78.91884872748643, CIDEr:5.197539789193376, Chrf:0.7880075784026452 chrf2:0.791452742171223
11/03/2021 01:21:12 - INFO - __main__ - Step 6400 Train loss 0.27 Learning rate 8.22e-07 BLEU 6062.71% on epoch=20
11/03/2021 01:21:12 - INFO - __main__ - 当前最好的：61.62
11/03/2021 01:27:21 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_32_6600.txt
11/03/2021 01:27:25 - INFO - __main__ - 当前的评估值: bleu-4:60.81864689091595, ROUGE_L:78.99915296022547, CIDEr:5.216344964426943, Chrf:0.788579839481325 chrf2:0.79242670649317
11/03/2021 01:27:25 - INFO - __main__ - Step 6600 Train loss 0.26 Learning rate 8.15e-07 BLEU 6081.86% on epoch=21
11/03/2021 01:27:25 - INFO - __main__ - 当前最好的：61.62
11/03/2021 01:33:34 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_33_6800.txt
11/03/2021 01:33:38 - INFO - __main__ - 当前的评估值: bleu-4:60.916188868596436, ROUGE_L:79.10799800555412, CIDEr:5.243266033587546, Chrf:0.789144761346077 chrf2:0.7928797363754538
11/03/2021 01:33:38 - INFO - __main__ - Step 6800 Train loss 0.26 Learning rate 8.09e-07 BLEU 6091.62% on epoch=21
11/03/2021 01:33:38 - INFO - __main__ - 当前最好的：61.62
11/03/2021 01:39:47 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_34_7000.txt
11/03/2021 01:39:51 - INFO - __main__ - 当前的评估值: bleu-4:60.690274533652946, ROUGE_L:79.02400428762823, CIDEr:5.204740204070042, Chrf:0.7876483023526324 chrf2:0.7910216673104279
11/03/2021 01:39:51 - INFO - __main__ - Step 7000 Train loss 0.26 Learning rate 8.02e-07 BLEU 6069.03% on epoch=22
11/03/2021 01:39:51 - INFO - __main__ - 当前最好的：61.62
11/03/2021 01:46:03 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_35_7200.txt
11/03/2021 01:46:06 - INFO - __main__ - 当前的评估值: bleu-4:60.295499879955315, ROUGE_L:78.57516520755271, CIDEr:5.144655529680715, Chrf:0.7863582324931828 chrf2:0.7897623256059818
11/03/2021 01:46:06 - INFO - __main__ - Step 7200 Train loss 0.26 Learning rate 7.95e-07 BLEU 6029.55% on epoch=23
11/03/2021 01:46:06 - INFO - __main__ - 当前最好的：61.62
11/03/2021 01:52:20 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_36_7400.txt
11/03/2021 01:52:23 - INFO - __main__ - 当前的评估值: bleu-4:60.23786550721767, ROUGE_L:78.64355380877912, CIDEr:5.156186898316515, Chrf:0.7852632484054425 chrf2:0.788881076219002
11/03/2021 01:52:23 - INFO - __main__ - Step 7400 Train loss 0.26 Learning rate 7.89e-07 BLEU 6023.79% on epoch=23
11/03/2021 01:52:23 - INFO - __main__ - 当前最好的：61.62
11/03/2021 01:58:30 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_37_7600.txt
11/03/2021 01:58:34 - INFO - __main__ - 当前的评估值: bleu-4:60.69636314003143, ROUGE_L:78.92636981120228, CIDEr:5.20216450926892, Chrf:0.7881040977859675 chrf2:0.7917363689900502
11/03/2021 01:58:34 - INFO - __main__ - Step 7600 Train loss 0.26 Learning rate 7.82e-07 BLEU 6069.64% on epoch=24
11/03/2021 01:58:34 - INFO - __main__ - 当前最好的：61.62
11/03/2021 02:04:39 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_38_7800.txt
11/03/2021 02:04:42 - INFO - __main__ - 当前的评估值: bleu-4:60.43167428947831, ROUGE_L:78.88315108067073, CIDEr:5.184006692964755, Chrf:0.7856546457002391 chrf2:0.7888647475811397
11/03/2021 02:04:43 - INFO - __main__ - Step 7800 Train loss 0.26 Learning rate 7.76e-07 BLEU 6043.17% on epoch=24
11/03/2021 02:04:43 - INFO - __main__ - 当前最好的：61.62
11/03/2021 02:10:59 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_39_8000.txt
11/03/2021 02:11:03 - INFO - __main__ - 当前的评估值: bleu-4:60.511965559349804, ROUGE_L:78.93321166809099, CIDEr:5.204002594243584, Chrf:0.7865121402041589 chrf2:0.7902617360394184
11/03/2021 02:11:03 - INFO - __main__ - Step 8000 Train loss 0.26 Learning rate 7.69e-07 BLEU 6051.20% on epoch=25
11/03/2021 02:11:03 - INFO - __main__ - 当前最好的：61.62
11/03/2021 02:17:16 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_40_8200.txt
11/03/2021 02:17:19 - INFO - __main__ - 当前的评估值: bleu-4:60.42263649569941, ROUGE_L:78.7217743958438, CIDEr:5.171019405273108, Chrf:0.785222747534192 chrf2:0.7886453490635189
11/03/2021 02:17:19 - INFO - __main__ - Step 8200 Train loss 0.26 Learning rate 7.62e-07 BLEU 6042.26% on epoch=26
11/03/2021 02:17:19 - INFO - __main__ - 当前最好的：61.62
11/03/2021 02:23:38 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_41_8400.txt
11/03/2021 02:23:41 - INFO - __main__ - 当前的评估值: bleu-4:60.1995688639328, ROUGE_L:78.61018892928085, CIDEr:5.162242476140037, Chrf:0.7860090775256336 chrf2:0.7894334666208298
11/03/2021 02:23:41 - INFO - __main__ - Step 8400 Train loss 0.25 Learning rate 7.56e-07 BLEU 6019.96% on epoch=26
11/03/2021 02:23:41 - INFO - __main__ - 当前最好的：61.62
11/03/2021 02:30:01 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_42_8600.txt
11/03/2021 02:30:04 - INFO - __main__ - 当前的评估值: bleu-4:60.72207104943544, ROUGE_L:79.06276335237658, CIDEr:5.218727824631653, Chrf:0.7878210846976801 chrf2:0.7913246950689502
11/03/2021 02:30:04 - INFO - __main__ - Step 8600 Train loss 0.26 Learning rate 7.49e-07 BLEU 6072.21% on epoch=27
11/03/2021 02:30:04 - INFO - __main__ - 当前最好的：61.62
11/03/2021 02:36:14 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_43_8800.txt
11/03/2021 02:36:18 - INFO - __main__ - 当前的评估值: bleu-4:60.15293857648596, ROUGE_L:78.67828339862307, CIDEr:5.178720641339612, Chrf:0.7841855251498484 chrf2:0.7878430989892675
11/03/2021 02:36:18 - INFO - __main__ - Step 8800 Train loss 0.26 Learning rate 7.43e-07 BLEU 6015.29% on epoch=28
11/03/2021 02:36:18 - INFO - __main__ - 当前最好的：61.62
11/03/2021 02:42:39 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_44_9000.txt
11/03/2021 02:42:43 - INFO - __main__ - 当前的评估值: bleu-4:59.96897320977571, ROUGE_L:78.63645375658945, CIDEr:5.152610989230053, Chrf:0.7828061193466405 chrf2:0.7865200896237581
11/03/2021 02:42:43 - INFO - __main__ - Step 9000 Train loss 0.25 Learning rate 7.36e-07 BLEU 5996.90% on epoch=28
11/03/2021 02:42:43 - INFO - __main__ - 当前最好的：61.62
11/03/2021 02:48:57 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_45_9200.txt
11/03/2021 02:49:01 - INFO - __main__ - 当前的评估值: bleu-4:60.40061279946294, ROUGE_L:78.90801940542005, CIDEr:5.202185026592675, Chrf:0.7860864191730348 chrf2:0.790190774267378
11/03/2021 02:49:01 - INFO - __main__ - Step 9200 Train loss 0.26 Learning rate 7.29e-07 BLEU 6040.06% on epoch=29
11/03/2021 02:49:01 - INFO - __main__ - 当前最好的：61.62
11/03/2021 02:55:11 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_46_9400.txt
11/03/2021 02:55:14 - INFO - __main__ - 当前的评估值: bleu-4:60.26576206039904, ROUGE_L:78.80306313285197, CIDEr:5.178760995064021, Chrf:0.7856565887557179 chrf2:0.7893315387220016
11/03/2021 02:55:14 - INFO - __main__ - Step 9400 Train loss 0.25 Learning rate 7.23e-07 BLEU 6026.58% on epoch=30
11/03/2021 02:55:14 - INFO - __main__ - 当前最好的：61.62
11/03/2021 03:01:31 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_47_9600.txt
11/03/2021 03:01:35 - INFO - __main__ - 当前的评估值: bleu-4:60.207320383500374, ROUGE_L:78.846455059347, CIDEr:5.184498445231204, Chrf:0.7846432789522595 chrf2:0.7884584239923985
11/03/2021 03:01:35 - INFO - __main__ - Step 9600 Train loss 0.25 Learning rate 7.16e-07 BLEU 6020.73% on epoch=30
11/03/2021 03:01:35 - INFO - __main__ - 当前最好的：61.62
11/03/2021 03:08:01 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_48_9800.txt
11/03/2021 03:08:04 - INFO - __main__ - 当前的评估值: bleu-4:60.131647256132126, ROUGE_L:78.85435753626851, CIDEr:5.172361716971453, Chrf:0.784175555005524 chrf2:0.7880127648810306
11/03/2021 03:08:04 - INFO - __main__ - Step 9800 Train loss 0.25 Learning rate 7.10e-07 BLEU 6013.16% on epoch=31
11/03/2021 03:08:04 - INFO - __main__ - 当前最好的：61.62
11/03/2021 08:26:23 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=200, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-05, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_4_best', no_lr_decay=False, num_beams=5, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_4_best/pos', predict_batch_size=16, predict_file='data/webnlg_ground/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=36, train_file='data/webnlg_ground/train', wait_step=30, warmup_proportion=0.01, warmup_steps=1000, weight_decay=0.0)
11/03/2021 08:26:23 - INFO - __main__ - out/jointgt_bart_webnlg_local
11/03/2021 08:26:23 - INFO - __main__ - Using 3 gpus
11/03/2021 08:26:23 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
11/03/2021 08:26:23 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
11/03/2021 08:26:23 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
11/03/2021 08:26:23 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
11/03/2021 08:26:23 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
11/03/2021 08:26:23 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
11/03/2021 08:26:23 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
11/03/2021 08:26:23 - INFO - transformers.tokenization_utils_base - loading file None
11/03/2021 08:26:23 - INFO - transformers.tokenization_utils_base - loading file None
11/03/2021 08:26:23 - INFO - transformers.tokenization_utils_base - loading file None
11/03/2021 08:26:23 - INFO - transformers.tokenization_utils_base - loading file None
11/03/2021 08:26:24 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_4_best/config.json
11/03/2021 08:26:24 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/03/2021 08:26:24 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_4_best/pytorch_model.bin
11/03/2021 08:26:33 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

11/03/2021 08:26:33 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_4_best.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
11/03/2021 08:26:33 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_4_best/pos/config.json
11/03/2021 08:26:33 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/03/2021 08:26:33 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_4_best/pos/pytorch_model.bin
11/03/2021 08:26:37 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BartForConditionalGeneration.

11/03/2021 08:26:37 - INFO - transformers.modeling_utils - All the weights of BartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_4_best/pos.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
11/03/2021 08:26:42 - INFO - __main__ - Starting training!
11/03/2021 08:32:23 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_0_200.txt
11/03/2021 08:32:27 - INFO - __main__ - 当前的评估值: bleu-4:62.05962452000685, ROUGE_L:79.70292855233645, CIDEr:5.344183362594936, Chrf:0.7951760147371962 chrf2:0.7985354499958487
11/03/2021 08:32:27 - INFO - __main__ - Step 200 Train loss 0.29 Learning rate 2.00e-07 BLEU 6205.96% on epoch=0
11/03/2021 08:32:27 - INFO - __main__ - 当前最好的：-1.00
11/03/2021 08:32:27 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/config.json
11/03/2021 08:32:32 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pytorch_model.bin
11/03/2021 08:32:32 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/pos/config.json
11/03/2021 08:32:36 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pos/pytorch_model.bin
11/03/2021 08:32:36 - INFO - __main__ - Saving model with best BLEU: -100.00% -> 6205.96% on epoch=0, global_step=200
11/03/2021 08:38:13 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_1_400.txt
11/03/2021 08:38:17 - INFO - __main__ - 当前的评估值: bleu-4:62.05162613240183, ROUGE_L:79.74429158913149, CIDEr:5.347333463285863, Chrf:0.7944980048197737 chrf2:0.7980679738157753
11/03/2021 08:38:17 - INFO - __main__ - Step 400 Train loss 0.29 Learning rate 4.00e-07 BLEU 6205.16% on epoch=1
11/03/2021 08:38:17 - INFO - __main__ - 当前最好的：62.06
11/03/2021 08:43:44 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_2_600.txt
11/03/2021 08:43:47 - INFO - __main__ - 当前的评估值: bleu-4:62.12652072662279, ROUGE_L:79.82953516294945, CIDEr:5.360420934440588, Chrf:0.7953992291226191 chrf2:0.7991474336605879
11/03/2021 08:43:47 - INFO - __main__ - Step 600 Train loss 0.29 Learning rate 6.00e-07 BLEU 6212.65% on epoch=1
11/03/2021 08:43:47 - INFO - __main__ - 当前最好的：62.06
11/03/2021 08:43:47 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_2/config.json
11/03/2021 08:43:52 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_2/pytorch_model.bin
11/03/2021 08:43:52 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_2/pos/config.json
11/03/2021 08:43:56 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_2/pos/pytorch_model.bin
11/03/2021 08:43:56 - INFO - __main__ - Saving model with best BLEU: 6205.96% -> 6212.65% on epoch=1, global_step=600
11/03/2021 08:49:31 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_3_800.txt
11/03/2021 08:49:35 - INFO - __main__ - 当前的评估值: bleu-4:62.146028553725955, ROUGE_L:79.7649529460188, CIDEr:5.362857971338093, Chrf:0.7954079736232563 chrf2:0.7990149325661617
11/03/2021 08:49:35 - INFO - __main__ - Step 800 Train loss 0.29 Learning rate 8.00e-07 BLEU 6214.60% on epoch=2
11/03/2021 08:49:35 - INFO - __main__ - 当前最好的：62.13
11/03/2021 08:49:35 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_3/config.json
11/03/2021 08:49:37 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_3/pytorch_model.bin
11/03/2021 08:49:37 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_3/pos/config.json
11/03/2021 08:49:38 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_3/pos/pytorch_model.bin
11/03/2021 08:49:38 - INFO - __main__ - Saving model with best BLEU: 6212.65% -> 6214.60% on epoch=2, global_step=800
11/03/2021 08:55:06 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_4_1000.txt
11/03/2021 08:55:10 - INFO - __main__ - 当前的评估值: bleu-4:62.25136396092237, ROUGE_L:79.8895497542557, CIDEr:5.376549219844566, Chrf:0.7962009551683971 chrf2:0.8000583356719949
11/03/2021 08:55:10 - INFO - __main__ - Step 1000 Train loss 0.28 Learning rate 1.00e-06 BLEU 6225.14% on epoch=3
11/03/2021 08:55:10 - INFO - __main__ - 当前最好的：62.15
11/03/2021 08:55:10 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_4/config.json
11/03/2021 08:55:11 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_4/pytorch_model.bin
11/03/2021 08:55:11 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_4/pos/config.json
11/03/2021 08:55:13 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_4/pos/pytorch_model.bin
11/03/2021 08:55:13 - INFO - __main__ - Saving model with best BLEU: 6214.60% -> 6225.14% on epoch=3, global_step=1000
11/03/2021 09:00:43 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_5_1200.txt
11/03/2021 09:00:47 - INFO - __main__ - 当前的评估值: bleu-4:62.049016591604115, ROUGE_L:79.84404892020038, CIDEr:5.353204036471548, Chrf:0.7949939777689407 chrf2:0.7984984287218494
11/03/2021 09:00:47 - INFO - __main__ - Step 1200 Train loss 0.29 Learning rate 9.93e-07 BLEU 6204.90% on epoch=3
11/03/2021 09:00:47 - INFO - __main__ - 当前最好的：62.25
11/03/2021 09:06:16 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_6_1400.txt
11/03/2021 09:06:20 - INFO - __main__ - 当前的评估值: bleu-4:61.92177014625923, ROUGE_L:79.71841121313365, CIDEr:5.33579701590247, Chrf:0.794254320033129 chrf2:0.7977696102154124
11/03/2021 09:06:20 - INFO - __main__ - Step 1400 Train loss 0.30 Learning rate 9.87e-07 BLEU 6192.18% on epoch=4
11/03/2021 09:06:20 - INFO - __main__ - 当前最好的：62.25
11/03/2021 09:11:51 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_7_1600.txt
11/03/2021 09:11:55 - INFO - __main__ - 当前的评估值: bleu-4:62.07489745828978, ROUGE_L:79.80650414574282, CIDEr:5.3641432940894775, Chrf:0.7953703576353905 chrf2:0.7990373843888836
11/03/2021 09:11:55 - INFO - __main__ - Step 1600 Train loss 0.29 Learning rate 9.80e-07 BLEU 6207.49% on epoch=5
11/03/2021 09:11:55 - INFO - __main__ - 当前最好的：62.25
11/03/2021 09:58:21 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=False, checkpoint='', clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=200, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=1e-05, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='bart', model_path='out/jointgt_bart_webnlg_local/run_4_best', no_lr_decay=False, num_beams=5, num_train_epochs=100.0, num_workers=8, output_dir='out/jointgt_bart_webnlg_local', pos_model_path='out/jointgt_bart_webnlg_local/run_4_best/pos', predict_batch_size=16, predict_file='data/webnlg_ground/test', prefix='save', remove_bos=False, save_period=500, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='pretrain_model/jointgt_bart', train_batch_size=36, train_file='data/webnlg_ground/train', wait_step=30, warmup_proportion=0.01, warmup_steps=1000, weight_decay=0.0)
11/03/2021 09:58:21 - INFO - __main__ - out/jointgt_bart_webnlg_local
11/03/2021 09:58:21 - INFO - __main__ - Using 3 gpus
11/03/2021 09:58:21 - INFO - transformers.tokenization_utils_base - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.
11/03/2021 09:58:21 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.
11/03/2021 09:58:21 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.
11/03/2021 09:58:21 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.
11/03/2021 09:58:21 - INFO - transformers.tokenization_utils_base - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.
11/03/2021 09:58:21 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/vocab.json
11/03/2021 09:58:21 - INFO - transformers.tokenization_utils_base - loading file pretrain_model/jointgt_bart/merges.txt
11/03/2021 09:58:21 - INFO - transformers.tokenization_utils_base - loading file None
11/03/2021 09:58:21 - INFO - transformers.tokenization_utils_base - loading file None
11/03/2021 09:58:21 - INFO - transformers.tokenization_utils_base - loading file None
11/03/2021 09:58:21 - INFO - transformers.tokenization_utils_base - loading file None
11/03/2021 09:58:22 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_4_best/config.json
11/03/2021 09:58:22 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "MyBartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/03/2021 09:58:22 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_4_best/pytorch_model.bin
11/03/2021 09:58:30 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.

11/03/2021 09:58:30 - INFO - transformers.modeling_utils - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_4_best.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.
11/03/2021 09:58:31 - INFO - transformers.configuration_utils - loading configuration file out/jointgt_bart_webnlg_local/run_4_best/pos/config.json
11/03/2021 09:58:31 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "extra_pos_embeddings": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/03/2021 09:58:31 - INFO - transformers.modeling_utils - loading weights file out/jointgt_bart_webnlg_local/run_4_best/pos/pytorch_model.bin
11/03/2021 09:58:35 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BartForConditionalGeneration.

11/03/2021 09:58:35 - INFO - transformers.modeling_utils - All the weights of BartForConditionalGeneration were initialized from the model checkpoint at out/jointgt_bart_webnlg_local/run_4_best/pos.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
11/03/2021 09:58:40 - INFO - __main__ - Starting training!
11/03/2021 10:04:19 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_0_200.txt
11/03/2021 10:04:23 - INFO - __main__ - 当前的评估值: bleu-4:62.27844515183409, ROUGE_L:79.88335234676532, CIDEr:5.368344328229675, Chrf:0.7958300244313183 chrf2:0.7989942208812117
11/03/2021 10:04:23 - INFO - __main__ - Step 200 Train loss 1.09 Learning rate 2.00e-07 BLEU 6227.84% on epoch=0
11/03/2021 10:04:23 - INFO - __main__ - 当前最好的：-1.00
11/03/2021 10:04:23 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/config.json
11/03/2021 10:04:24 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pytorch_model.bin
11/03/2021 10:04:24 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_0/pos/config.json
11/03/2021 10:04:25 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_0/pos/pytorch_model.bin
11/03/2021 10:04:25 - INFO - __main__ - Saving model with best BLEU: -100.00% -> 6227.84% on epoch=0, global_step=200
11/03/2021 10:10:01 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_1_400.txt
11/03/2021 10:10:05 - INFO - __main__ - 当前的评估值: bleu-4:62.285015212679426, ROUGE_L:79.9380213019256, CIDEr:5.37832481346005, Chrf:0.7985993475420846 chrf2:0.8012239041335292
11/03/2021 10:10:05 - INFO - __main__ - Step 400 Train loss 0.56 Learning rate 4.00e-07 BLEU 6228.50% on epoch=1
11/03/2021 10:10:05 - INFO - __main__ - 当前最好的：62.28
11/03/2021 10:10:05 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_1/config.json
11/03/2021 10:10:07 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_1/pytorch_model.bin
11/03/2021 10:10:07 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_1/pos/config.json
11/03/2021 10:10:08 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_1/pos/pytorch_model.bin
11/03/2021 10:10:08 - INFO - __main__ - Saving model with best BLEU: 6227.84% -> 6228.50% on epoch=1, global_step=400
11/03/2021 10:15:42 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_2_600.txt
11/03/2021 10:15:46 - INFO - __main__ - 当前的评估值: bleu-4:62.707849449479966, ROUGE_L:80.13995803417411, CIDEr:5.409573115621934, Chrf:0.7996981969994373 chrf2:0.802148195346952
11/03/2021 10:15:46 - INFO - __main__ - Step 600 Train loss 0.36 Learning rate 6.00e-07 BLEU 6270.78% on epoch=1
11/03/2021 10:15:46 - INFO - __main__ - 当前最好的：62.29
11/03/2021 10:15:46 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_2/config.json
11/03/2021 10:15:48 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_2/pytorch_model.bin
11/03/2021 10:15:48 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_2/pos/config.json
11/03/2021 10:15:49 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_2/pos/pytorch_model.bin
11/03/2021 10:15:49 - INFO - __main__ - Saving model with best BLEU: 6228.50% -> 6270.78% on epoch=1, global_step=600
11/03/2021 10:22:17 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_3_800.txt
11/03/2021 10:22:21 - INFO - __main__ - 当前的评估值: bleu-4:62.686295451789405, ROUGE_L:80.0858147480777, CIDEr:5.404228700843747, Chrf:0.7993035477929217 chrf2:0.8017874944491161
11/03/2021 10:22:21 - INFO - __main__ - Step 800 Train loss 0.35 Learning rate 8.00e-07 BLEU 6268.63% on epoch=2
11/03/2021 10:22:21 - INFO - __main__ - 当前最好的：62.71
11/03/2021 10:30:33 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_4_1000.txt
11/03/2021 10:30:36 - INFO - __main__ - 当前的评估值: bleu-4:62.6606189138536, ROUGE_L:80.15422393610066, CIDEr:5.410725355328355, Chrf:0.7990223438096002 chrf2:0.8020391547299364
11/03/2021 10:30:36 - INFO - __main__ - Step 1000 Train loss 0.35 Learning rate 1.00e-06 BLEU 6266.06% on epoch=3
11/03/2021 10:30:36 - INFO - __main__ - 当前最好的：62.71
11/03/2021 10:38:36 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_5_1200.txt
11/03/2021 10:38:40 - INFO - __main__ - 当前的评估值: bleu-4:62.981886999568545, ROUGE_L:80.33467692303513, CIDEr:5.442608979071135, Chrf:0.800282227782518 chrf2:0.8029642119572582
11/03/2021 10:38:41 - INFO - __main__ - Step 1200 Train loss 0.34 Learning rate 9.93e-07 BLEU 6298.19% on epoch=3
11/03/2021 10:38:41 - INFO - __main__ - 当前最好的：62.71
11/03/2021 10:38:41 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_5/config.json
11/03/2021 10:38:42 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_5/pytorch_model.bin
11/03/2021 10:38:42 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_5/pos/config.json
11/03/2021 10:38:44 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_5/pos/pytorch_model.bin
11/03/2021 10:38:44 - INFO - __main__ - Saving model with best BLEU: 6270.78% -> 6298.19% on epoch=3, global_step=1200
11/03/2021 10:46:44 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_6_1400.txt
11/03/2021 10:46:48 - INFO - __main__ - 当前的评估值: bleu-4:63.096923127017, ROUGE_L:80.42261365541373, CIDEr:5.456512950932144, Chrf:0.8011179592084137 chrf2:0.8038299096124518
11/03/2021 10:46:48 - INFO - __main__ - Step 1400 Train loss 0.34 Learning rate 9.87e-07 BLEU 6309.69% on epoch=4
11/03/2021 10:46:48 - INFO - __main__ - 当前最好的：62.98
11/03/2021 10:46:48 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_6/config.json
11/03/2021 10:46:50 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_6/pytorch_model.bin
11/03/2021 10:46:50 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_6/pos/config.json
11/03/2021 10:46:51 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_6/pos/pytorch_model.bin
11/03/2021 10:46:51 - INFO - __main__ - Saving model with best BLEU: 6298.19% -> 6309.69% on epoch=4, global_step=1400
11/03/2021 10:54:58 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_7_1600.txt
11/03/2021 10:55:02 - INFO - __main__ - 当前的评估值: bleu-4:63.35604554792981, ROUGE_L:80.50215288663921, CIDEr:5.481989797242957, Chrf:0.8023929691691511 chrf2:0.8051364705755737
11/03/2021 10:55:02 - INFO - __main__ - Step 1600 Train loss 0.33 Learning rate 9.80e-07 BLEU 6335.60% on epoch=5
11/03/2021 10:55:02 - INFO - __main__ - 当前最好的：63.10
11/03/2021 10:55:02 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_7/config.json
11/03/2021 10:55:04 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_7/pytorch_model.bin
11/03/2021 10:55:04 - INFO - transformers.configuration_utils - Configuration saved in out/jointgt_bart_webnlg_local/run_7/pos/config.json
11/03/2021 10:55:05 - INFO - transformers.modeling_utils - Model weights saved in out/jointgt_bart_webnlg_local/run_7/pos/pytorch_model.bin
11/03/2021 10:55:05 - INFO - __main__ - Saving model with best BLEU: 6309.69% -> 6335.60% on epoch=5, global_step=1600
11/03/2021 11:03:05 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_8_1800.txt
11/03/2021 11:03:09 - INFO - __main__ - 当前的评估值: bleu-4:63.08650368348269, ROUGE_L:80.36553206342013, CIDEr:5.444399834433095, Chrf:0.8004860856678645 chrf2:0.802963090923105
11/03/2021 11:03:09 - INFO - __main__ - Step 1800 Train loss 0.33 Learning rate 9.74e-07 BLEU 6308.65% on epoch=5
11/03/2021 11:03:09 - INFO - __main__ - 当前最好的：63.36
11/03/2021 11:11:09 - INFO - __main__ - Saved prediction in out/jointgt_bart_webnlg_local/savepredictions_9_2000.txt
11/03/2021 11:11:13 - INFO - __main__ - 当前的评估值: bleu-4:63.08254363881489, ROUGE_L:80.419830005578, CIDEr:5.452973491930508, Chrf:0.8006068445844674 chrf2:0.8034833283986549
11/03/2021 11:11:13 - INFO - __main__ - Step 2000 Train loss 0.33 Learning rate 9.67e-07 BLEU 6308.25% on epoch=6
11/03/2021 11:11:13 - INFO - __main__ - 当前最好的：63.36
